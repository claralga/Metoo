{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kV9SWKZSWAh0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 16:09:19.746222: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-10 16:09:22.015761: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "#from transformers import AutoModelForSequenceClassification, CamembertForMaskedLM, AutoTokenizer, AutoConfig\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "# Importing specific libraries for data prerpcessing, model archtecture choice, training and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pADt5CykCKaz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def labeliser_tweet(df, nb_tweets=5, random_state=0):\n",
    "    \n",
    "    # V√©rifier si le fichier \"label.csv\" existe et charger les tweet_id d√©j√† labelis√©s\n",
    "    tweets_labelises = set()\n",
    "    try:\n",
    "        with open('label.csv', mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                tweets_labelises.add(row['tweet_id'])\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "    # S√©lectionner un √©chantillon al√©atoire de tweets non encore labelis√©s\n",
    "    df_a_labeliser = df[~df.index.isin(tweets_labelises)].sample(n=nb_tweets, random_state=random_state)\n",
    "    \n",
    "    # Labeliser les tweets s√©lectionn√©s et enregistrer les labels dans un fichier CSV\n",
    "    with open('label.csv', mode='a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if f.tell() == 0:\n",
    "            writer.writerow(['tweet_id', 'text', 'label'])\n",
    "        for tweet_id, text in df_a_labeliser['text'].items():\n",
    "            label = input(f'Label pour le tweet suivant :\\n{text}\\n')\n",
    "            # √âcrire les donn√©es labelis√©es dans le fichier CSV\n",
    "            writer.writerow([tweet_id, text, label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "fGUP7YlcanuW",
    "outputId": "9d6da307-a96e-45dc-ca7f-acd015856f90",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_concat_clean/base_annotation_fev23.csv', index_col='tweet_id').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKjOXzht-kvr",
    "outputId": "18bd519c-92df-4ce6-c15c-ec4555f02a8c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label pour le tweet suivant :\n",
      "Affaire #Ad√®leHaenel : le r√©alisateur #ChristopheRuggia plac√© en garde √† vue #MeToo https://t.co/6daXmWhTqK\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "Un pain au chocolat peut-il permettre de se faire pardonner une agression sexuelle? #Haziza #balancetonporc https://t.co/g8YGxBzeJM\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@CordaniOfficiel Comme je vous comprends....j'ai le m√™me probl√®me avec le mois de f√©vrier....courage #MeToo #MeTooInceste #metoosuisse #suisse #inceste #silence #maltraitance\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Publication livre : \"Ni pantins ni soumis\" ou les exc√®s du #METOO  https://t.co/B0YvW5GyXA @libgallimard https://t.co/PcE29MWuPs\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "*‚í∏‚í∏‚û• #Venezuela ‚òõ Communist psychopaths | https://t.co/iFKJd7x2Kb | #Residente #Ren√©P√©rez #UnicefComunista #Unicef #JairBolsonaro #FernandoAlb√°n #Brasil #BrieLarson #BenicioDelToro #MeToo #AngelinaJolie #ACNUR #CateBlanchett #LGBT #Comunismo #Anticomunismo https://t.co/XmQppraaNU\n",
      "0\n",
      "Label pour le tweet suivant :\n",
      "@briefmenews en effet, vs avez raison. Tant mieux, √ßa me permet de continuer √† vs lire avec confiance. Suoer r√©cap sur l'Alg√©rie ds le dernier Brief. Je compte sur vs pr faire un aussi bon brief sur #MeToo et #IDidntReport\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Un budget qui va titiller le scrotum des fran√ßais, sans les toucher, normal √ßa fait bien longtemps qu'ils n'en n'ont plus ! #metoo https://t.co/PUFgeryO88\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Il y a des ≈ìuvres qui au del√† de leurs beaut√©s ont une force cathartique societale po√©tique indispensable Merci @AndreaBescond  #ericmetayer  la reconstruction est possible #metoo #JeSuisVictime #psychanalyse #stopviolencesfaitesauxenfants #imprescriptibilit√© https://t.co/RCedpIqc3O :QT: En 2016, un ovni d√©barquait au Petit @TMontparnasse. Ovationn√© chaque soir par un public boulevers√©.  Couronn√© par le Moli√®re du seul/e en sc√®ne, #LESCHATOUILLES d‚Äô@AndreaBescond, mise en sc√®ne @Metayereric, est exceptionnellement en ligne jusqu‚Äôau 3/05 !  https://t.co/vriKkmkAZI\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#metoo Alors pour 16 -18, en ce moment precis, (3:07 le 25/07/202i) je ne sait pas les appeler. Je suis nul en orthographe et fort en grammaire quand j'√©tais √† l '√©cole primaire. je ne sais pas pourquoi je dis cela.\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "VIDEO TPMP People : vifs √©changes sur le plateau autour du mouvement #MeToo https://t.co/XT1pzDczgG https://t.co/JFOpEcFB7A\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "#balancetonporc : on d√©mon les porcs sans d√©noncer la porcherie #RTLMatin @MadeJessey #SensCommun #LR\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@MarleneSchiappa @EPhilippePM @NadiaHAI78 @MarleneSchiappa  Quand allez-vous porter secours aux enseignantes violent√©es par leur  chef d'√©tablissement et aux lanceurs d'alerte  r√©prim√©s par @EducationFrance @justice_gouv ?   Merci de cesser de conforter l'Omerta.   #metoo #PasdeVague #NousToutes   https://t.co/LXngHxIYrA\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Absolution accord√©e √† un agresseur sexuel : le DPCP fera appel https://t.co/pTxhlpwpjS #Qu√©bec #polqc #simonhoule #TroisRivieres #AgressionNonDenoncee #MeToo #JusticeQc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "The Les concern√©s Daily est en ligne! https://t.co/57GQC6LQR8 #afp #metoo\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "@Frambwazz @doucefrance13 @MonsieurAbdool La diff√©rence c'est que le BalanceTonYoutubeur c'est que le youtubeur il se sert de sa notori√©t√© et de la faiblesse psychologique de ses fans. Un random c'est dans #BalanceTonPorc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Sarah Palin Compares Sacha Baron Cohen Duping Politicians to #MeToo https://t.co/GuJdeW4Ohr https://t.co/DjVxDR68uK\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "Le gouvernement est aveugle aux aspirations de la jeune g√©n√©ration #MeToo, par Les invit√©s de Mediapart via @MediapartBlogs https://t.co/95htIrxLHZ\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "@tweekette75 @MaitreKlem @HutzLionel1 @Brujulasegunda @Le___Doc @immaterielle @sui_gene_ris @Kdet_Rousselle Je ne pense pas avoir √©crit √† un seul moment LES Gyneco dans leur ensemble. LA profession de Gyneco. Je visais ce monsieur et ces comparses.  mais r√©pondre #NotAllGyneco quand on parle probl√®mes IVG et violences Gyneco est pour moi du m√™me niveau que #NotAllMen contre #MeToo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#MeToo : le studio Weinstein vendu √† une soci√©t√© d'investissement https://t.co/mD8XellyGu via @Culturebox\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "En 93 d√©j√†, 72% des femmes avait subi un acte de #harc√®lement sexuel dit L. Bertalli #RTSinfrarouge #MeToo https://t.co/9yzIJgkAXD\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Le mec de 50 ans au bar qui m'a dit que je mettais du rouge √† l√®vres uniquement pour allumer les mecs #balancetonporc\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "Je me suis toujours dit, elles ont fait un #BalanceTonPorc , qu'est ce qui se serai pass√© si on avait fait un #BalanceTaPute ?\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@ginmedaddy Dans ce cas va √©crire sur #BalanceTonPorc. A moins que ton pr√™tre est une pouffe ce que j'en doute?  Vous les femme vous √™te toutes les m√™me, vous ne RESPECTEZ RIEN #BalanceTonPorc=Injustice faite aux femmes par les hommes #BalanceTaPouffe=Injustice faite aux hommes par les femme\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "avec #MeToo on parle de crimes : viols, agressions sexuelles etc. On parle de choses qui sont punissables par la loi (m√™me si on sait tous-tes ce que √ßa donne) du coup je vois mal comment on peut parler de d√©lation alors qu'on d√©nonce des pratiques punissables\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#WaltDisney - Le prince de \"La Belle au bois dormant\" est-il un pr√©dateur sexuel ? https://t.co/jDbKD9RHJI via @franceculture #Jeunesse #Conte #F√©minisme #BalanceTonPorc https://t.co/IvGS6GD1Jn\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "#MeToo #metooinceste et #MeTooGay offrent aussi une opportunit√© de penser l‚Äôoppression que peut repr√©senter le silence et le pouvoir de la parole (mais aussi ses limites). La parole lib√®re mais elle n‚Äôefface pas l‚Äôirr√©vocable d‚Äôun acte terrible. Elle est cri de justice !\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "J'√©coute #Jablonka depuis 10 min, il y a rien qui va. Le mec il n'a pas d√ª lire un seul truc √©crit par des f√©ministes depuis #metoo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Accus√© d'harc√®lement sexuel, le maire de Copenhague d√©missionne en pleine nouvelle vague de #Metoo au Danemark https://t.co/ZPYKZXneHs via @rtbfinfo\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "#balancetonporc un nouveau hashtag √† la #agressionnondenoncee\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "le nombre de #Iwas ca me brise le coeur\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@GG_RMC @DICKENSDAVID1 et la peur ? tu connais ? elle a peut √™tre trouv√© le courage de d√©noncer en voyant le mouvement actuel #balancetonporc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Ceci EST une agression sexuelle.  Aucun d√©bat. #balancetonporc https://t.co/NYJl1aguRV\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#balancetonporc j'ai √©t√© viol√© par le chien de Michel Drucker\n",
      "0\n",
      "Label pour le tweet suivant :\n",
      "Baisse des cr√©dits du programme 137. Annonces du Grenelle non financ√©es. Manque de moyens des associations malgr√© la hausse de leur activit√© cons√©cutive √† #metoo. La v√©rit√© sur les #bullshit du gouvernement, dans le rapport de la tr√®s rigoureuse commission des finances du Senat! https://t.co/hTxKV5olvF :QT: La commission des finances du S√©nat assez s√®che sur \"le milliard d'euros\" pour l'√©galit√© femmes hommes https://t.co/VK7nBOt2BZ https://t.co/p9Bv5jiX5A\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "En France on as vraiment un probl√©me.On est compl√©tement passer √† cot√© du mouvement #MeToo et on as donn√© le c√©sar √† Roman Polanski. Et l√† on est sur le point de passer √† cot√©, de ce mouvement historique de justice contre le racisme. Je comprends vraiment plus rien √† ce pays.\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Apr√®s #MeToo / #BalanceTonPorc autour de #BalanceTonYoutubeur qui fait exploser #twitter ! Encore une fois faut prendre du recule pour ne pas faire les m√™mes erreurs qui ont d√©j√† √©t√© commise. #Squeezie :QT: Les YouTubers (y compris ceux qui crient sur tous les toits qu‚Äôils sont f√©ministes) qui profitent de la vuln√©rabili‚Ä¶ https://t.co/B7VGErsR2z\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@ATrapenard @Giulia_Fois_ @fabricearfi @marineturchi  Merci de RTüôè La #p√©docriminalit√© est un fl√©au avec descons√©quences colossales sur la vie des #victimes. Besoin de #politiques publiques √† la hauteur. #Iwas #StopPrescription St‚õîÔ∏èp #ViolencesSexuelles https://t.co/MtGmQlr7T1\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Waw ces anecdotes gravissimes sur ce HT #balancetonporc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Le secret d'une vie bien men√©e c'est qu'elle soit bien remplie. #denoncetonporc #denoncetatruie #balancetonporc #Metoo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "\"Je me suis vraiment sentie salie\" : paroles d'√©ditrices √† propos des violences sexistes et sexuelles dans le milieu litt√©raire https://t.co/717XebvMbp via @franceinfo #sexisme #f√©minisme #ViolenceMasculine #metoo #BalanceTonPorc\n",
      "\n",
      "Label pour le tweet suivant :\n",
      "#metoo #metooinceste #CONSISTENCY  UN JEU DIFF√âRENCE COULEUR/TEXTE  https://t.co/hA0D41vxZz\n",
      "0\n",
      "Label pour le tweet suivant :\n",
      "@ericrevel1 Et en cette p√©riode #metoo, un tel obs√©d√© aurait pris cher. Toutes les qualit√©s ce mec.\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#balancetonporc alors au lieu de demander des noms, rendez-vous compte du nombre de t√©moignages, de l'horreur quotidienne que c'est.\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Indescencence: croire que ne pas √™tre agresseur m√©rite d'√™tre c√©l√©br√©...  #balancetonporc https://t.co/vHsE3rYsx2\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@s_cluzel @N_Hulot Bravo! Encore une fois, notre gouvernement marque sa bonne marche dans la gestion des affaires publiques et de la s√©paration des pouvoirs. #mercipourcemoment #BalanceTonPorc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@ellensalvi En somme #metoo a raison quand ses accusateurs (trices) accusent. Et #metoo a raison quand ses acusateurs publics (trices/ques) sont demasqu√©(e)s. Quoi qu‚Äôil arrive, #metoo a raison.\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "https://t.co/Ee9Dpul3ph Des m√©thodes vraiment pr√© #MeToo pour acquitter les flics accus√©s du #violDu36 : fouiller dans la vie de la victime pour la d√©nigrer, l'accuser d'√™tre ivre, l'accuser de confusion (stress post-trauma). Le #patriarcat a la vie dure, surtout dans la justice.\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@CBouanchaud Quelqu‚Äôun pour m‚Äôexpliquer pourquoi il y a toujours un moment de repr√©sentation hypersexualis√©e ? Avec #metoo, la parole et une r√©flexion en profondeur sur les actes er la soci√©t√© semblaient prendre le dessus, mais on revient √† des demonstration sexualis√©e. Pige pas.\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Les valeurs 2 @Zemmour ce n'est pas celles de notre r√©publique mais bien celles du fais ce que je dis mais pas ce que je fais Coucher avec son conseill√®re qui est sa subordonn√©e n'est pas r√©publicain  #MeTooPolitique #balancetonporc Pens√©e √† sa femme victime du grand remplacement :QT: #Mohamed VS #Zemmour   ‚û°Ô∏è Plus d‚Äôun million de vues sur mon compte #TikTok (https://t.co/huGJi7dGp4).   Merci ! https://t.co/8JYA7SB9Uo\n",
      "0\n",
      "Label pour le tweet suivant :\n",
      "Super doc post #metoo en pr√©paration pour LCP, suivez le compte sur insta ! (Et pi c'est une copine)  https://t.co/QVxkB0zReI\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "labeliser_tweet(df, nb_tweets=50, random_state = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jAyzaJ5875dw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_label = pd.read_csv('label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "mRUK1kmZ7_z8",
    "outputId": "78d2f214-ea47-47f1-d388-9679b8d6d384",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2.0    828\n",
       "1.0    270\n",
       "3.0    113\n",
       "0.0     42\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_label = df_label.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hnt-ro2_WpXz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining constants\n",
    "epochs = 10\n",
    "MAX_LEN = 300\n",
    "batch_size = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-JvbX630WUHn",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89b418bcc7a451bafbe2cd902834364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)tencepiece.bpe.model:   0%|          | 0.00/811k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e772c93523cb48b8a869f49574739852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f574914b8f4115a1a855f739e921d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertForSequenceClassification(\n",
       "  (roberta): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): CamembertClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=4)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=10e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "cQS2l-oQZwmL",
    "outputId": "ab347867-6f13-41ee-9457-1ea3ef877c2f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "text = df_label['text'].to_list()\n",
    "labels = df_label['label'].to_list()\n",
    "\n",
    "#user tokenizer to convert sentences into tokenizer\n",
    "input_ids  = [tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN) for sent in text]\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]  \n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "Ge4RCSKcZztV",
    "outputId": "47bd58fd-bb54-45c5-b011-e4af769fcee9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "                                                            random_state=42, test_size=0.4)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "pbFcqTLYWnIZ",
    "outputId": "ca5a0099-dba7-4b4e-ad98-14b46dbb3819",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2618875404198964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|‚ñà         | 1/10 [11:58<1:47:46, 718.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.683666087962963\n",
      "Train loss: 1.0668712556362152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|‚ñà‚ñà        | 2/10 [26:31<1:47:53, 809.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.683666087962963\n",
      "Train loss: 0.957703024148941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|‚ñà‚ñà‚ñà       | 3/10 [41:12<1:38:15, 842.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.683666087962963\n",
      "Train loss: 0.8441829631725947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [56:13<1:26:33, 865.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7936197916666666\n",
      "Train loss: 0.7229040463765463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [1:11:12<1:13:06, 877.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7316261574074074\n",
      "Train loss: 0.6163275490204493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [1:25:21<57:51, 867.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7779947916666666\n",
      "Train loss: 0.5189388146003088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [1:38:30<42:06, 842.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8236400462962963\n",
      "Train loss: 0.4161665042241414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [1:46:59<24:32, 736.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8298611111111112\n",
      "Train loss: 0.3496982256571452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [1:54:01<10:37, 637.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8383969907407407\n",
      "Train loss: 0.2971478613714377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [2:00:39<00:00, 723.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8423032407407407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss_set=[]\n",
    "for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels.long())\n",
    "        loss = outputs[0]\n",
    "        train_loss_set.append(loss.item())    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    model.eval()\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs =  model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels.long())\n",
    "            loss, logits = outputs[:2]\n",
    "    \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8221643 , -0.83890676,  2.5219278 , -0.92354465],\n",
       "       [-0.81028825, -1.07352   ,  2.5322819 , -0.725595  ],\n",
       "       [-0.34867215,  2.070702  , -0.7203709 , -0.78563815],\n",
       "       [-0.38904977,  2.0407073 , -0.5774643 , -0.8829936 ],\n",
       "       [-0.37330586,  2.0593083 , -0.645782  , -0.8296578 ],\n",
       "       [-0.36106187,  2.0633984 , -0.67290175, -0.81339943],\n",
       "       [-0.8287327 , -0.91648006,  2.538202  , -0.8618867 ],\n",
       "       [-0.80881274, -1.1790465 ,  2.467678  , -0.56627357],\n",
       "       [-0.47179356,  2.0046434 , -0.36589116, -0.96706545],\n",
       "       [ 0.13611896, -0.89000213, -0.81778085,  1.5121919 ],\n",
       "       [-0.5510757 , -1.6220843 ,  1.6150688 ,  0.41074413],\n",
       "       [-0.8388688 , -0.76494545,  2.5143547 , -0.95420045],\n",
       "       [-0.81536543, -0.94629   ,  2.535512  , -0.8548476 ],\n",
       "       [-0.82336545, -0.97191536,  2.546595  , -0.82897425],\n",
       "       [-0.87656933, -0.49813908,  2.4661756 , -1.1080488 ],\n",
       "       [-0.8113716 , -0.9549724 ,  2.5452538 , -0.85571337],\n",
       "       [-0.313241  , -1.5654745 ,  0.90972686,  0.85591567],\n",
       "       [-0.82185245, -0.77548176,  2.534844  , -0.9789212 ],\n",
       "       [-0.31588787,  2.0782387 , -0.8131522 , -0.74371976],\n",
       "       [-0.906727  ,  0.136479  ,  2.1084023 , -1.2957884 ],\n",
       "       [-0.3466072 ,  2.0709176 , -0.7266443 , -0.78978086],\n",
       "       [ 0.12745321, -0.8740188 , -0.8265506 ,  1.5019445 ],\n",
       "       [ 0.09549239, -0.9439918 , -0.70322037,  1.4987872 ],\n",
       "       [ 0.11192477, -0.9146199 , -0.7643703 ,  1.5037094 ],\n",
       "       [-0.7893951 , -0.93905354,  2.5125606 , -0.8641126 ],\n",
       "       [-0.79454863, -0.90420926,  2.535768  , -0.89576244],\n",
       "       [-0.7494637 , -1.2723875 ,  2.459231  , -0.56383413],\n",
       "       [-0.8151687 , -0.96693426,  2.5525303 , -0.83956957],\n",
       "       [-0.796139  , -0.920967  ,  2.541124  , -0.8812912 ],\n",
       "       [-0.61143947, -1.6351812 ,  1.8655634 ,  0.21725975],\n",
       "       [-0.3131526 ,  2.0723882 , -0.81363904, -0.73718274],\n",
       "       [-0.7997458 , -1.1139518 ,  2.4308772 , -0.60173714],\n",
       "       [-0.35464805,  2.0689168 , -0.7289603 , -0.7766812 ],\n",
       "       [-0.5733671 ,  1.7986631 ,  0.08574091, -1.1468596 ],\n",
       "       [-0.4635352 ,  1.9970679 , -0.3719347 , -0.93919075],\n",
       "       [-0.7964984 , -0.9275209 ,  2.5338733 , -0.87363356],\n",
       "       [-0.73766196, -1.4382079 ,  2.2940345 , -0.25399253],\n",
       "       [-0.81251603, -1.0391923 ,  2.5322132 , -0.7587744 ],\n",
       "       [-0.92036116,  0.24218182,  2.039402  , -1.2951918 ],\n",
       "       [-0.74385905,  0.12416483,  1.9623332 , -1.2434186 ],\n",
       "       [-0.9162116 ,  0.5680897 ,  1.8605716 , -1.3983872 ],\n",
       "       [-0.8068203 , -1.0083487 ,  2.5225716 , -0.8004366 ],\n",
       "       [-0.7893088 , -0.95427823,  2.5423083 , -0.85736024],\n",
       "       [-0.91950834,  0.3575328 ,  1.992028  , -1.3718396 ],\n",
       "       [-0.34458727,  2.0724235 , -0.7603166 , -0.7614318 ],\n",
       "       [-0.8525001 , -0.9911487 ,  2.5273418 , -0.75805795],\n",
       "       [ 0.06329463, -1.0494696 , -0.56660277,  1.4499758 ],\n",
       "       [-0.8466078 , -0.71145797,  2.5273762 , -0.9838972 ],\n",
       "       [-0.33588907,  2.0747738 , -0.7833482 , -0.747755  ],\n",
       "       [-0.91969585,  0.37546128,  1.8632278 , -1.2724297 ],\n",
       "       [-0.33518684,  2.0616288 , -0.72089446, -0.8081205 ],\n",
       "       [-0.80449706, -0.78376436,  2.533194  , -0.97836703],\n",
       "       [-0.78339136, -1.0119371 ,  2.5355659 , -0.82826006],\n",
       "       [-0.787206  ,  0.14527246,  1.9581571 , -1.2546949 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kgii8LE3Y6_N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
