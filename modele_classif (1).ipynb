{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "kV9SWKZSWAh0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "#from transformers import AutoModelForSequenceClassification, CamembertForMaskedLM, AutoTokenizer, AutoConfig\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "# Importing specific libraries for data prerpcessing, model archtecture choice, training and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "pADt5CykCKaz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def labeliser_tweet(df, nb_tweets=5, random_state=0):\n",
    "    \n",
    "    # V√©rifier si le fichier \"label.csv\" existe et charger les tweet_id d√©j√† labelis√©s\n",
    "    tweets_labelises = set()\n",
    "    try:\n",
    "        with open('label.csv', mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                tweets_labelises.add(row['tweet_id'])\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "    # S√©lectionner un √©chantillon al√©atoire de tweets non encore labelis√©s\n",
    "    df_a_labeliser = df[~df.index.isin(tweets_labelises)].sample(n=nb_tweets, random_state=random_state)\n",
    "    \n",
    "    # Labeliser les tweets s√©lectionn√©s et enregistrer les labels dans un fichier CSV\n",
    "    with open('label.csv', mode='a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if f.tell() == 0:\n",
    "            writer.writerow(['tweet_id', 'text', 'label'])\n",
    "        for tweet_id, text in df_a_labeliser['text'].items():\n",
    "            label = input(f'Label pour le tweet suivant :\\n{text}\\n')\n",
    "            # √âcrire les donn√©es labelis√©es dans le fichier CSV\n",
    "            writer.writerow([tweet_id, text, label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "fGUP7YlcanuW",
    "outputId": "9d6da307-a96e-45dc-ca7f-acd015856f90",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_concat_clean/base_annotation_fev23.csv', index_col='tweet_id').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKjOXzht-kvr",
    "outputId": "18bd519c-92df-4ce6-c15c-ec4555f02a8c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label pour le tweet suivant :\n",
      "Homme femelle N¬∞19: ü§≥\"Fais tr√®s vite mon amour, car tu dois ensuite aller lui les torcher... Ta ch√©rie qui t'aime\".  #feminisme #metoo #violencesconjugales https://t.co/wEyZ2ucc0c\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "Proc√®s de #balancetonporc : Sandra Muller d√©nonce ‚Äúune inversion des r√¥les o√π le pr√©dateur devient la proie, la proie devient le pr√©dateur\"  ‚û° https://t.co/tXIFSsR12K https://t.co/law5XfRHTz\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "@canalplus, une r√©action officielle peut-√™tre sur les r√©v√©lations visant @PierreMenes ?... #PierreMenesOut #balancetonporc   #BalanceTonMenes #JeNeSuisPasUneSalope https://t.co/ParThES1UG\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "‚ÄúJ‚Äôai pouss√© un cri de col√®re‚Äù : jug√©e en diffamation, la journaliste Sandra Muller √† l‚Äôorigine de #Balancetonporc plaide ‚Äúla lib√©ration de la parole des femmes‚Äù https://t.co/nHix95DBmq\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "https://t.co/wFK3baer5V ‚Ä¶ dossier sur anissa jbr (achat de follower ,consomation de stup escorting) #jeremstarcensure   #BalanceTonPorc   #jeremstarcensure #babybel    #LaVilla3 #fiansosurskyrock  #JeremstarGate  #BALANCETONBABYBEL  #BalanceTonPorc  #Balancetatruie  #LPEPDLA\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@RC_ML pour continuer sur la vague de ce matin, la merveilleuse Nadia Daam @zappette parle, elle aussi, de son scepticisme de voir Mme Deneuve dans le m√©tro √† 8h du mat. #moiaussi #MeToo #BalanceTonPorc https://t.co/Ns8vVKXAS0 :QT: \"Les am√©ricains ont Oprah Winfrey et des d√©bats sur Woody Allen, nous on a Elisabeth L√©vy, des d√©fenseurs du droit inali√©nable √† coller des mains au cul, et des d√©bats sur les blagues de Tex !\"  R√©√©coutez le billet de Nadia Daam (@Zappette) #E1matin ‚¨áÔ∏è https://t.co/92svLYh4Vj\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@sychazot Et comme elle ne va pas √™tre nomm√©e elle va encore instrumentaliser #metoo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@TPZ_Rap ... elle d√©nonce des choses ce qui a un lien avec le #balancetonporc ... la comparaison est pas n√©cessaire bref\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "On se demandera aussi si celles qui ont us√© de leurs atouts via promotion canap√© pour un poste parleront..#balancetonporc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "√ßa critique Pierre M√®nes mais √ßa klaxonne, siffle et traite les femmes quand √ßa passe en bande dans une Peugeot 206 üíÄüíÄüíÄ #PierreMenesOut #TPMP #balancetonporc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@SDJ_LCP a vot√© suite √† la direction de remettre @frhaz √† l'antenne .. √©trangement √ßa ne choque pas beaucoup de monde .. c'est fini #Balancetonporc ? O√π √ßa vaut que pour certains #Haziza https://t.co/EOFckFfNbp\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Plaintes pour violences sexuelles (par le dessinateur Schvartz) -&gt; https://t.co/ymSfoZ7Wok #actualit√©endessins #JM #LOL #violencessexuelles #agressionssexuelles #violencesfaitesauxfemmes #harc√®lementsexuel #paroledesfemmes #balanceTonPorc #schvartz #weinsteinscandal\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "#Gis√®leHalimi √©tait heureuse de voir qu'il y avait des ferments de r√©volte un peu partout   Exemple : #MeToo  Annick Cojean  Cl√©a @Zenon8703 https://t.co/z3svClWRO6\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Vous pouvez √©galement le faire au commissariat ou gendarmerie le/la plus proche de chez vous #balancetonporc https://t.co/qWsG8KMISx\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Erdoƒüan le Dictateur islamiste !!! Insulte et amalgame volontaire avec des groupuscules terroristes. Erdoƒüan les combats depuis des lustres alors que les pr√©c√©dents les caressaient dans le sens du poil sans aucun gain !  #BalanceTonPastƒ±rma (contre attaque √† #BalanceTonPorc)\n",
      "0\n",
      "Label pour le tweet suivant :\n",
      "Un peu d'humour #balancetonporc üòÇ https://t.co/eL4JX56m0f\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@FanJeanDujardin C bizarre de revoir #OSS117 au Br√©sil et ses r√©pliques sexistes qq mois apr√®s #BalanceTonPorc .. Mais l'humour a tjs raison\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#BalanceTonPorc Squeezie avait donc raison\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@raphpradeau L'essentiel est qu'on laisse dans l'opacit√© le #MobbingEducationNationale et les violences sexuelles commises par des principaux et des proviseurs.  #metoo #moiaussi   #PasDeVague #OmertaEducatioNationale  https://t.co/ffGjK9kDQM\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#MeToo  √Ä celui qui m'a embrass√© et frapp√©.\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "Regardez le point de vue de @grosfilley et l'interview d'Anne Morelli, l'une des signataires de la tribune publi√©e hier dans Le Monde qui d√©nonce un certain f√©minisme qui exprimerait \"une certaine haine des hommes\" https://t.co/hxjPAAQ43T #Jour1 #MeToo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "RT @hrw au sujet du mouvement #MeToo en #Iran, o√π des voix s'√©l√®vent pour demander une loi + forte concernant les violences contre les #femmes. https://t.co/W3yVudkzJq\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Ta copine de bac √† sable t'a vol√© ta pelle quand t'avais 3 ans? Ta voisine de classe t'a pas laiss√© copier sur sa dict√©e? Ta copine t'a quitt√© alors que vous vous √©tiez promis l'amour √©ternel √† 14 ans? D√©foule toi sur #Balancetapouffe OSEF. Les vrais pb sont l√† -&gt; #balancetonporc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Ce que #metoo a chang√© pour les journalistes https://t.co/j1bwq1bcz9\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "#MetooAmnesie  #metooinceste  Tu aimes ton metier, tu es oblig√© de t arr√™ter + de 3 mois car tu as peur, naus√©e, oppression, panique, flash, non-reconnaissance des gens en crises, concentration faible Et que ce n est pas de TA FAUTE! C est insupportable! SVP, Qui est concern√© ?\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Le chef du bureau de liaison d‚ÄôIsra√´l au Maroc d√©ment les accusations d‚Äôabus sexuels #metoo https://t.co/TFeRW5lOPH\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "M√™me Habill√© Avec Une Tunique XXL, Le Pervers Cherchera Toujours A Mater. #BalanceTonPorc\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "Ensemble, sortons du silence ! Appelez d√®s maintenant le 0805 802 804 ou le 0800 100 811 depuis l'outre-mer du lundi au vendredi de 10h √† 19h. T√©moignez pour vous. T√©moigner pour les enfants victimes. T√©moignez, tout simplement. @CIIVISE_contact  #Metooinceste #ViolenceSexuelles https://t.co/NoJky5WvM4\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Grande diff√©rence toutefois dans cette √©dition 2018, qui √©tait davantage plac√©e sous le slogan #MeToo, plut√¥t que #ResistTrump. https://t.co/9aSHEs85xZ\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Via @euronewsfr : Le producteur Harvey Weinstein exclu de l'Acad√©mie des Oscars https://t.co/WRV4LYbazT =C‚Äôest tout ?#balancetonporc\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "@AlexDevecchio @PascalPraud le bin√¥me inf√¢me. 2 ideologues d‚Äôextreme droite raciste islamophobe. Pas content que ‚Äòles femmes fran√ßaises musulmanes n‚Äôaient pas utilis√© le hashtag #balancetonporc ‚Äò car elles auraient peur de se faire √©gorg√©s: ils sont journalistes, jamais inqui√®t√©s https://t.co/C5DtnSXJBb :QT: üî¥üá´üá∑#FRANCE : Mensonges d'@AlexDevecchio (journaliste au #Figaro) qui dit, je cite \"elles se font √©gorg√©es en bas de leurs cit√©s\".üßê #Mytho https://t.co/ORqkILKwn8\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#MeToo lib√®re la parole en #Egypte o√π le harc√®lement sexuel reste encore tabou, explique le #NobelAlternatif @Mozn https://t.co/lgdzteIrhU\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "#balancetonporc  #balancetonporc qu'elle ambiance quand les hommes ne feront que r√™ver au bon vieux temps...\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#balancetonporc s'habiller en jean basket ds ton village car du + jeune au +++ √¢g√© tu es un morceau de viande!!! √Ä vomir!\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "@TheDracerGx et m√™me d‚Äôun buddy movie avec Samuel L. Jackson, sur fond de revendications f√©ministes pour marquer un peu plus Hollywood dans l‚Äô√®re #MeToo.\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "On ne voit plus jm maire #tpmp petit probl√®me avec #balancetonporc?\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Chrronique de @sophiedurocher qui pisse sur le cadavre encore chaud d‚Äôun p√®re de 3 enfants.   Sophie veut surtout pas qu‚Äôon oublie #metoo dans le cas de Kobe mais elle est beaucoup plus silencieuse sur la cas de l‚Äôami Rozon. Les vacances √† Paris √ßa forge des liens solides. https://t.co/gTXYYtqhR1\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#YoutubeusesDay ! Bravo mesdames !!! Svp (et je m'adresse aux femmes un minimum intelligentes). Vs v√©hiculez une influence sur les jeunes. Pourriez vs faire de la pr√©vention contre la #pedocriminalite dans vos vid√©os/reseaux ? @innocencedanger #iwas   https://t.co/1PkpS3jKLJ\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "celles et ceux qui tweetent sur le #Iwas vous √™tes tlmt fort sachez le, vous m√©ritez d‚Äô√™tre entendus\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Excellente analyse sur le tr√®s entendu \"#MeToo c'est bien, mais...\" https://t.co/tQvyCQX4wu\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Je rentre dans un magazin le gerant me suit \"pour me parler des produits \"et se met √† me toucher les seins #balancetonporc\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "C'est une agression sexuelle embraser une pets contre son consentement espece de porc @PierreMenes !!! √áa toujours √©t√© une agression sexuelle complice inf√¢me @canalplus ü§Æü§Æü§Æü§Æ #PierreMenesOut  #balancetonporc https://t.co/uALtjVQpoX :QT: Ah bon j'ai pas le droit de soulever des jupes et d'embrasser des femmes de forces ? Oh la la, le monde a chang√© ! Gros d√©gueulasse va ü§Æ #TPMP #PierreMenesOut\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Marche aussi pour les mecs qui r√¢lent sur #balancetonporc https://t.co/8jj3aTp0mb\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Un magnifique projet tr√®s bient√¥t thanks @ambreleguilly thanks @noemie.de.lattre hairstyle and makeup @nallahsangare #feminist #journeeinternationaledesdroitsdesfemmes #metoo #memepaspeur #resilience #sorority #model #ivorycoast #senegal #cotesdarmor #actress #writer #director https://t.co/LCm46glsJI\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#joyeusespaques #premieravril #jeremstargates #balancetonporc  #sondage vous faites quoi en premiers quand vous avez un kider surprise(follow,je fais des sondages)\n",
      "0\n",
      "Label pour le tweet suivant :\n",
      "@arthurberdah @ccornevin @AlbertZennou Il faut le stopper lui surtout le caillou dans la chaussure de Macron. √Ä chaque fois qu‚Äôil ouvrira la bouche on pensera √† ses services rendus contre du sexe. #balancetonporc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Colo kayak, seule fille, une nuit tout le groupe avec les mono, mega groping sur moi dans mon duvet. J'ai jamais rien dit.  #balancetonporc\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "Bref, cette interview, n√©cessaire  √† mon sens, a confort√© mon avis sur cette affaire. √áa n'en fait √©videmment pas un coupable, au mieux un mauvais communiquant en revanche. #PPDA #Quotidien @Qofficiel #MeToo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "√Ä un moment donn√©, les r√©alisateurs tv vont arr√™ter de faire des plans en contreplong√©e pour mater sous les jupes des filles ? ü§î #TPMP #BalanceTonPorc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Certaines ne parlent pas de peur de ¬´ d√©truire la vie ¬ª bien en place de bon p√®re de famille ins√©r√© de l‚Äôagresseur, cela peut peut √™tre para√Ætre √©tonnant mais c‚Äôest un sentiment courant, cela renvoie d‚Äôailleurs aux propos tr√®s d√©plac√©s qui ont eu cours lors de l‚Äô√©pisode #metoo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@Space_Marcel @RHvitserk @crue_carmen @Le___Doc Ce qu'il faut que vous compreniez c'est le discours f√©ministe. Vous ne reconnaissez pas le syst√®me patriarcal dans lequel on vit ni la perspective de genre et forc√©ment, vous ne pouvez avoir aucune compr√©hension du mouvement #metoo autre que: elles veulent la guerre des sexes\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Catherine Deneuve et Brigitte Bardot moqu√©es dans une √©mission am√©ricaine pour leurs positions sur #BalanceTonPorc https://t.co/gBQHbErWkR\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "Cor√©e du Sud: une irr√©ductible de Samsung est la championne #MeToo https://t.co/JTEMJELsXg\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "#balancetonporc C'est pas gentil pour les cochons... üê∑üêñ\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#metoo oops #mytho...\"avant √ßa passait cr√®me Cyril\" duxit Pierre Men√©s sur @Cyrilhanouna https://t.co/nx14Rz0ydT :QT: üá´üá∑ FLASH - Pierre #M√©n√®s se d√©fend ce soir dans #TPMP : ¬´ Il faut prendre les gens comme ils sont, j'ai √©t√© embauch√© parce que je suis un personnage ¬ª. ¬´ Si je ne peux plus chambrer une meuf parce que c'est une meuf, c'est insupportable ¬ª, a-t-il ajout√©. (interview) https://t.co/QLeIzByREW\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@PCF Pourquoi le parquet / procureur n'a pas poursuivi le violeur ?  Les juges doivent r√©pondre de leurs actes !   #responsabilit√© #pedocriminalit√© #metoo #balancetonporc #viols #Gailhaguet #justice\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@f_letellier @jeanvaljean2018 @GwennB_ Certes, apr√®s, il y a des gens qui se consacrent √† plus d√©fendre, les femmes, ou les enfants ou, d autres, des associations qui se d√©vouent pour leur cause sans pour autant remettre en doute la parole des uns ou des autres comme √ßa a √©t√© le cas pour #MeToo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Le #Iwas il me brise le coeur, vous √™tes tous/tes fort(e)s et extr√™mement courageux(ses), je vous souhaite le meilleur ü•∫‚ù§Ô∏è\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Le poulet a merite les coups de poings qu'il a pris car desole mais on est pas la pour prendre des coups donc ce sera coup pour coup La police a baisser dans l'estime des francais. #libertepourdettinger #balancetonporc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Merci √† @AbouDjaffar de RT #balancetonporc, √ßa m'√©vite de cliquer sur le hashtag et d'en d√©couvrir encore plus‚Ä¶ #naus√©e\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@ta_malou @avec_marine Elle aime les enfants la preuve elle s en es occup√© aux lyc√© de cette nouille de #macron brizitte devrais figur√© dans #BalanceTonPorc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "A song against Violence / Un titre qui vous prend aux tripes \"Violent &amp; Happy\" de La Nouvelle Eve sur #SoundCloud ? #np https://t.co/FZiTWLPrlZ #violence #violenceconjugale #violenceathome #stayhome #music #resteralamaison #covid #corona #covid19 #bjork #bj√∂rk #metoo\n",
      "0\n",
      "Label pour le tweet suivant :\n",
      "Les t√©moignages du #MetooInceste sont juste gla√ßants. Les pr√©dateurs sexuels ne sont jamais tr√®s loin .\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Donc le collectif #MeToo d√©montre sa b√™tise abyssale et son manque de discernement. üòâ #QuandOnEstConOnEstCon https://t.co/DUGBqqLgqa\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@Lotisauteur @Mediapart Oui. J'ai √©t√© contact√© pour @BalanceTonPost et @TPMP notament et √† l'√©tranger pour parler de #MeToo. Quand je parlerai, je parlerai aussi de √ßa\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Je viens de l'√©couter. Encore une fois, excellente intervention de @Ovidieofficiel qui a raison sur la vraie pollution du d√©bat (plus que n√©cessaire) ouvert par les #MeToo #Balancetonporc #TIMESUP .  Cette tribune du Monde ne fait que brouiller/salir toute vraie discussion. https://t.co/MVfkNqm3eB\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Ce matin, on rigole bien sur FB. Et tout √ßa avant mon 2e caf√© #metoo #enitalien https://t.co/w6oA8YIHDo\n",
      "0\n",
      "Label pour le tweet suivant :\n",
      "Vatican. La derni√®re bataille de Beno√Æt XVI - Courrier International Blogs https://t.co/ILVd5Ry3XU #metoo https://t.co/5SoPW4oqaG\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "#metoo #metooinceste  \"prophet\":  4min24sec, pendant quelques secondes, il est comme De Niro parfois:   COMPLIQUE: je vous ecoute: je vous donne le pouvoir d'etre celui qui DOIT vous ecouter  A 16 ans, mes amis disait que j'etais comme lui dans \"Voyage au bout de l'enfer\",PARFOIS https://t.co/p42cEijVul\n",
      "0\n",
      "Label pour le tweet suivant :\n",
      "Si il est prouv√© que Sandra Muller  @LettreAudio  A DE GROS SEINS, il demeure incompr√©hensible qu'Eric Brion ai eu envie de se taper cette grosse truie #balancetonporc #journeemondialedesanimaux\n",
      "√©\n",
      "Label pour le tweet suivant :\n",
      "‚ÄúEn allant vers l'h√©micycle,@jeanlassalle m'a mis une main aux fesses.‚ÄúJulia Castanier se confie. #balancetonporc https://t.co/2ybbTRj5sq\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "@WiretteNicorett @fabfluckiger Quand tu dis que √ßa se travaille, √† quel degr√© il est n√©cessaire de commencer un traitement ? Et avec qui ?  #MeToo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Quand on te demande ton 06 dans la rue, que tu d√©clines ou ignores pour finalement te faire insulter de pute... üò© #balancetonporc\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "√Ä 21h, le #doc ¬´ Beauvoir, l‚Äôaventure d‚Äô√™tre soi ¬ª  üë©‚Äçüè´ √Ä l‚Äôheure des d√©bats sur la domination masculine, la charge mentale, #MeToo, quelle est la modernit√© de Simone de Beauvoir ?  üëâ Avec Le√Øla Slimani, Elisabeth Badinter et @titiou Lecoq  R√©al Fabrice Gardel et @MathieuWeschler https://t.co/laqKrJzFBi\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Nouvelle plainte pour #viol contre #Tariqramadan #Ramadan #balancetonporc https://t.co/GMJT3S1oBM\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Donc chacun ne voit pas la m√™me chose concernant 1er moiti√© du James Bond et deuxi√®me moiti√© du film loque humaine merci #metoo https://t.co/3PLromOFdX\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@Bloom12005028 Formidable merci. Oui il faut imp√©rativement tout faire pour r√©former la l√©gislation sur le #viol au #Japon qui d√©j√† n'avait pas √©t√© modifi√©e, il y a peu, depuis 110 ans... #Scandale #MeToo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Le cas Aziz Ansari questionne intelligemment le mouvement #MeToo https://t.co/XWWojlDtJ2 https://t.co/hpL0NMZqwd\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "@CNEWS @angele_vl Une vraie rebelle cette meuf ! üòÜ Chanson sur #balancetonporc pr surfer sur la vague, chanson sur le r√©chauffement en pleine canicule wow. J'annonce un prochain titre sur la p√™che au homard et sur les vignettes Crit'Air mdrrr #risquez√©ro\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "\"Preuve que les questions li√©es aux femmes ont domin√© l‚Äôann√©e, les mots-clefs les plus utilis√©s sur Twitter en 2018 place ¬´ school #MeToo ¬ª, ¬´ feminist ¬ª et ¬´ molka ¬ª\".  Cet article date d'il y a un an, qu'en est-il pour 2019? ü§î #digipolUCLouvain https://t.co/FEyFo3NoDW\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#balancetonporc ce soir dans derni√®re minute\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#Gilead ne l√¢che pas l‚Äôaffaire contre @raoult_didier  √áa fait pls mois que c un feu roulant continu. Mais un truc comme √ßa garantit des dommages collat√©raux ou alors il faudra un tri s√©lectif des d√©nonciations anonymes (DAs). Apr√®s les #metoo, Les DAs sont maintenant la norme. ü•∂ https://t.co/U9MZGxZ56K :QT: Dites-moi que c'est un fake! https://t.co/EM3WkFHBqo Y aurait-il des fonctionnaires visant des promotions en essayant de se faire bien voir en incitant √† la d√©lation.Opposer m√©decins et pharmaciens, pas forc√©ment une strat√©gie gagnante, et en tout cas pas au service des patients. https://t.co/ikc6v1MHSX\n",
      "0\n",
      "Label pour le tweet suivant :\n",
      "#Iwas 15, 16, 17, deux fois le m√™me gros connard\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "2.2- #metoo #metooinceste   NOTES P√âDOPHILES  Il a fallu que je me d√©barrasse de le personne qui a peut-√™tre le plus de nonSoi que je connaisse et qui me pose beaucoup trop de probl√®me: j'ai d√©mont√© son ramassis de pr√©tention et d'apparences ridicules.\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Agression et harc√®lement sexuels: le photographe star Patrick Demarchelier accus√©, Anna Wintour mise en cause https://t.co/4Z3wnQTXWy #metoo #Demarchelier #Wintour https://t.co/M2Kbd93WtK\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "Un an apr√®s #MeToo, retrouvons-nous pour dire üõë STOP aux violences  sexistes et sexuelles. RDV samedi 29 septembre, √† 14h30, place de la  R√©publique, pour un die-in g√©ant.  üìÖ √âv√©nement Facebook : https://t.co/eVUsV1tczD üëâ Inscriptions : https://t.co/y0dJaQCalu https://t.co/gNnUP7YzXt\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#balancetonporc, √ßa balance pas mal √† Paris, √ßa balance pas m√¢le‚Ä¶ https://t.co/EFyUSn9Pf8 https://t.co/LlpDsgYjbH\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "J'avais 2 ans. C'√©tait le fils d'un ami de la famille : il en avait entre 11 et 13. Je l'ai recrois√© il y a 3 ans : il a maintenant 40 ans et m'a regard√© en mode \"tu te souviens\" avec son regard de pervers.  J'ai port√© la culpabilit√© de son acte pendant 30 ans. #metooinceste\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "Avec mon b√©b√© de 8 mois dans son landeau.Un type approche:c'est une fille ou un gar√ßon? Moi:une fille.Lui:une future suceuse #balancetonporc\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "Le pr√™tre de Trois-Rivi√®res a tent√© de mettre fin √† ses jours - RCI https://t.co/8Zsk7oQnz9 #metoo https://t.co/r0dkl00Pla\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "Des suffragettes √† #MeToo : le violet, couleur iconique des mouvements f√©ministes https://t.co/Jeu0Cb216y @letemps #soc\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "#Metoo Les f√©ministes radicales de gauche de plus en plus critiqu√©es par des...f√©ministes....https://t.co/hQLAbzqHDg via @lp_lapresse\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "@YannisPSD @_Ecce__Homo_ @MarleneSchiappa \"#S√©n√©galüá∏üá≥ : #Macronüá´üá∑ et #Rihanna saluent un nouvel √©lan pour l‚Äô√©ducation\"  #Schiappa @Elysee #BrigitteMacron #France #Paris #LREM #EnMarche #LaREM #BalanceTonPorc https://t.co/BPJXRAnkNa\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Hanouna avec capu anav et lors de l‚Äôentretien d embauche d enora #balancetonporc@hanouna@c8\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "üî¥Notre tribune dans @libe  ‚û°Ô∏è\"Violences Sexuelles sur mineur.e.s : l‚Äôheure de la tol√©rance z√©ro a sonn√©\"üîî   #justicepourjulie #metooinceste  Avec @memoiretrauma @MiKohiyama @AndreaBescond @Abitbol_sarah @corinne_leriche @aludv P. √â. Germain Thill  üìå https://t.co/tSxr6LHm28 https://t.co/qpTnlxJ2BS\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "https://t.co/Ezz6jd3MjO Oui je l'adore üíôüéµüé∂ Je ne suis pas #MeToo https://t.co/QTzSFXWAFH\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "@MiKohiyama Pas que dans le sport, pas que dans le cin√©ma, les porcs ne sont pas tous sous les projecteurs mais ils sont partout dans nos vies de tous les jours, tout pr√®s et les victimes aussi...üò¢#metoo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Pourquoi certaines enqu√™tes sont-elles d'utilit√© publique ? Pour que des anonymes ne s'autorisent plus de traiter des victimes potentielles de \"femelles\" hyst√©riques. Les enqu√™tes permettent d'ouvrir des enqu√™tes, qui d√©montrerons ou non la culpabilit√© ou l'innocence. #MeToo https://t.co/06RIyrKGBG\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Bordel j'suis dans l'EHPAD de mon grand-p√®re.. putain si les soignantes participaient au hashtag #balancetonporc mdddddr √ßa aurait saturer Twitter\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#√©v√©nement R√©putation Time 2017 : du tweet #balancetonporc √† la couverture du Elle, anatomie d‚Äôune lev√©e de boucliers g√©n√©ralis√©e. C'est demain avec @annatwit #RepTime2017  En savoir plus &gt; https://t.co/Z0nGlUq457  Programme et inscription &gt; https://t.co/gP3b4GM9CP https://t.co/V6BsQrFcnR\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "labeliser_tweet(df, nb_tweets=100, random_state = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "jAyzaJ5875dw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_label = pd.read_csv('label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_label = df_label.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_label = df_label.drop(1979)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il n'y a pas de cha√Ænes de caract√®res avec des lettres dans la colonne 'label'.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Filtrer les cha√Ænes de caract√®res contenant des lettres dans la colonne \"label\"\n",
    "has_strings = df_label['label'].apply(lambda x: bool(re.search('[a-zA-Z]', str(x))))\n",
    "\n",
    "if has_strings.any():\n",
    "    # Supprimer les lignes contenant des cha√Ænes de caract√®res avec des lettres dans la colonne \"label\"\n",
    "    df_label = df_label[~has_strings]\n",
    "    print(\"Les lignes contenant des cha√Ænes de caract√®res avec des lettres ont √©t√© supprim√©es.\")\n",
    "else:\n",
    "    print(\"Il n'y a pas de cha√Ænes de caract√®res avec des lettres dans la colonne 'label'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_label.label = [int(label) for label in df_label.label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "mRUK1kmZ7_z8",
    "outputId": "78d2f214-ea47-47f1-d388-9679b8d6d384",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    1331\n",
       "1     412\n",
       "3     182\n",
       "0      81\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "hnt-ro2_WpXz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining constants\n",
    "epochs = 50\n",
    "MAX_LEN = 300\n",
    "batch_size = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "-JvbX630WUHn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertForSequenceClassification(\n",
       "  (roberta): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): CamembertClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=4)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=10e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "cQS2l-oQZwmL",
    "outputId": "ab347867-6f13-41ee-9457-1ea3ef877c2f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "text = df_label['text'].to_list()\n",
    "labels = df_label['label'].to_list()\n",
    "\n",
    "#user tokenizer to convert sentences into tokenizer\n",
    "input_ids  = [tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN) for sent in text]\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]  \n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "Ge4RCSKcZztV",
    "outputId": "47bd58fd-bb54-45c5-b011-e4af769fcee9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "                                                            random_state=42, test_size=0.4, stratify = labels)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9748553665060746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|‚ñè         | 1/50 [05:49<4:45:30, 349.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6665178571428572\n",
      "Train loss: 0.8132788131111547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   4%|‚ñç         | 2/50 [12:09<4:54:09, 367.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7814903846153847\n",
      "Train loss: 0.636475898717579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   6%|‚ñå         | 3/50 [19:07<5:05:42, 390.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.832760989010989\n",
      "Train loss: 0.504281925527673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   8%|‚ñä         | 4/50 [25:38<4:59:30, 390.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8774038461538461\n",
      "Train loss: 0.39367974118182536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|‚ñà         | 5/50 [32:15<4:54:46, 393.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8393543956043956\n",
      "Train loss: 0.32853785941475316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  12%|‚ñà‚ñè        | 6/50 [38:56<4:50:15, 395.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.860989010989011\n",
      "Train loss: 0.27336769041262177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  14%|‚ñà‚ñç        | 7/50 [45:21<4:41:03, 392.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8718063186813187\n",
      "Train loss: 0.23118420023667186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  16%|‚ñà‚ñå        | 8/50 [51:37<4:30:57, 387.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8882211538461539\n",
      "Train loss: 0.207986234050048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  18%|‚ñà‚ñä        | 9/50 [58:02<4:23:58, 386.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8762019230769231\n",
      "Train loss: 0.17864405167730232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|‚ñà‚ñà        | 10/50 [1:04:24<4:16:40, 385.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8573832417582418\n",
      "Train loss: 0.14990130459007464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  22%|‚ñà‚ñà‚ñè       | 11/50 [1:10:39<4:08:20, 382.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8657967032967033\n",
      "Train loss: 0.14182989220870168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  24%|‚ñà‚ñà‚ñç       | 12/50 [1:17:14<4:04:27, 385.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8918269230769231\n",
      "Train loss: 0.1237568037682458\n",
      "Train loss: 0.10782157904223393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  28%|‚ñà‚ñà‚ñä       | 14/50 [1:29:59<3:50:10, 383.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8657967032967033\n",
      "Train loss: 0.11725765270622153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|‚ñà‚ñà‚ñà       | 15/50 [1:35:32<3:34:52, 368.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8778159340659341\n",
      "Train loss: 0.11720255762338638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [1:42:14<3:34:30, 378.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.817135989010989\n",
      "Train loss: 0.10115535459236095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [1:48:49<3:51:15, 408.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8740041208791209\n",
      "Early stopping! No improvement in validation loss for 10 consecutive epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss_set = []\n",
    "best_validation_loss = float('inf')\n",
    "epochs_no_improve = 0  # nombre d'epochs cons√©cutives sans am√©lioration de la validation\n",
    "best_model = None  # stockage du meilleur mod√®le\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels.long())\n",
    "        loss = outputs[0]\n",
    "        train_loss_set.append(loss.item())    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "    # Evaluate on validation data\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels.long())\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "    \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "            nb_eval_examples += b_input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "\n",
    "    # Check for early stopping\n",
    "    validation_loss = eval_loss/nb_eval_steps\n",
    "    if validation_loss < best_validation_loss - 0.01:\n",
    "        best_validation_loss = validation_loss\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        # Enregistrer le meilleur mod√®le\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve == 10:\n",
    "        print(\"Early stopping! No improvement in validation loss for 10 consecutive epochs.\")\n",
    "        break\n",
    "\n",
    "# Enregistrer le meilleur mod√®le sur le disque\n",
    "if best_model is not None:\n",
    "    torch.save(best_model, \"best_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(\"best_model.pth\", \"classif_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertForSequenceClassification(\n",
       "  (roberta): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): CamembertClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=4)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    # mettre le mod√®le en mode d'√©valuation\n",
    "    model.eval()\n",
    "    \n",
    "    # stocker les pr√©dictions de tous les batchs dans une liste\n",
    "    predictions = []\n",
    "    \n",
    "    # boucle sur les batches dans le dataloader de test\n",
    "    for batch in dataloader:\n",
    "        \n",
    "        # d√©placer les donn√©es sur le m√™me dispositif que le mod√®le\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # d√©sactiver le calcul des gradients pour √©conomiser de la m√©moire et acc√©l√©rer les calculs\n",
    "        with torch.no_grad():\n",
    "            # faire passer les donn√©es √† travers le mod√®le pour obtenir les logits\n",
    "            outputs =  model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = outputs[0]\n",
    "    \n",
    "        # appliquer la fonction softmax pour obtenir les probabilit√©s pour chaque classe\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # convertir les probabilit√©s en pr√©dictions en prenant l'indice de la classe avec la probabilit√© la plus √©lev√©e\n",
    "        batch_preds = torch.argmax(probs, dim=1)\n",
    "        \n",
    "        # ajouter les pr√©dictions pour ce batch √† la liste de pr√©dictions globale\n",
    "        predictions.extend(batch_preds.cpu().numpy().tolist())\n",
    "    \n",
    "    # retourner la liste de pr√©dictions globale\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_val = predict(model, validation_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(validation_labels.numpy(), return_counts=True)\n",
    "label_counts = dict(zip(unique, counts))\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(validation_labels, predictions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = ['Autre','Presse','Opinion','T√©moignage'], \n",
    "                     columns = ['Autre','Presse','Opinion','T√©moignage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_df, annot=True, cbar = False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('')\n",
    "plt.xlabel('Pr√©dictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Charger les donn√©es et les mettre au format appropri√©\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[43mtexts\u001b[49m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "# Charger les donn√©es et les mettre au format appropri√©\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "# Inf√©rer les pr√©dictions et les probabilit√©s de chaque classe\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs[0]\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    preds = torch.argmax(logits, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter une colonne de pr√©dictions\n",
    "df[\"predictions\"] = preds.cpu().numpy()\n",
    "\n",
    "# Ajouter des colonnes de probabilit√©s pour chaque classe\n",
    "for i, label in enumerate(label_list):\n",
    "    df[label] = probs[:, i].cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "pbFcqTLYWnIZ",
    "outputId": "ca5a0099-dba7-4b4e-ad98-14b46dbb3819",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2618875404198964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|‚ñà         | 1/10 [11:58<1:47:46, 718.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.683666087962963\n",
      "Train loss: 1.0668712556362152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|‚ñà‚ñà        | 2/10 [26:31<1:47:53, 809.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.683666087962963\n",
      "Train loss: 0.957703024148941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|‚ñà‚ñà‚ñà       | 3/10 [41:12<1:38:15, 842.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.683666087962963\n",
      "Train loss: 0.8441829631725947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [56:13<1:26:33, 865.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7936197916666666\n",
      "Train loss: 0.7229040463765463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [1:11:12<1:13:06, 877.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7316261574074074\n",
      "Train loss: 0.6163275490204493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [1:25:21<57:51, 867.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7779947916666666\n",
      "Train loss: 0.5189388146003088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [1:38:30<42:06, 842.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8236400462962963\n",
      "Train loss: 0.4161665042241414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [1:46:59<24:32, 736.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8298611111111112\n",
      "Train loss: 0.3496982256571452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [1:54:01<10:37, 637.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8383969907407407\n",
      "Train loss: 0.2971478613714377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [2:00:39<00:00, 723.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8423032407407407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### sans early stopping\n",
    "train_loss_set=[]\n",
    "for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels.long())\n",
    "        loss = outputs[0]\n",
    "        train_loss_set.append(loss.item())    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    model.eval()\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs =  model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels.long())\n",
    "            loss, logits = outputs[:2]\n",
    "    \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
