{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "kV9SWKZSWAh0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "#from transformers import AutoModelForSequenceClassification, CamembertForMaskedLM, AutoTokenizer, AutoConfig\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "# Importing specific libraries for data prerpcessing, model archtecture choice, training and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "pADt5CykCKaz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def labeliser_tweet(df, nb_tweets=5, random_state=0):\n",
    "    \n",
    "    # VÃ©rifier si le fichier \"label.csv\" existe et charger les tweet_id dÃ©jÃ  labelisÃ©s\n",
    "    tweets_labelises = set()\n",
    "    try:\n",
    "        with open('label.csv', mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                tweets_labelises.add(row['tweet_id'])\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "    # SÃ©lectionner un Ã©chantillon alÃ©atoire de tweets non encore labelisÃ©s\n",
    "    df_a_labeliser = df[~df.index.isin(tweets_labelises)].sample(n=nb_tweets, random_state=random_state)\n",
    "    \n",
    "    # Labeliser les tweets sÃ©lectionnÃ©s et enregistrer les labels dans un fichier CSV\n",
    "    with open('label.csv', mode='a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if f.tell() == 0:\n",
    "            writer.writerow(['tweet_id', 'text', 'label'])\n",
    "        for tweet_id, text in df_a_labeliser['text'].items():\n",
    "            label = input(f'Label pour le tweet suivant :\\n{text}\\n')\n",
    "            # Ã‰crire les donnÃ©es labelisÃ©es dans le fichier CSV\n",
    "            writer.writerow([tweet_id, text, label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "fGUP7YlcanuW",
    "outputId": "9d6da307-a96e-45dc-ca7f-acd015856f90",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_concat_clean/base_annotation_fev23.csv', index_col='tweet_id').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKjOXzht-kvr",
    "outputId": "18bd519c-92df-4ce6-c15c-ec4555f02a8c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label pour le tweet suivant :\n",
      "Homme femelle NÂ°19: ðŸ¤³\"Fais trÃ¨s vite mon amour, car tu dois ensuite aller lui les torcher... Ta chÃ©rie qui t'aime\".  #feminisme #metoo #violencesconjugales https://t.co/wEyZ2ucc0c\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "ProcÃ¨s de #balancetonporc : Sandra Muller dÃ©nonce â€œune inversion des rÃ´les oÃ¹ le prÃ©dateur devient la proie, la proie devient le prÃ©dateur\"  âž¡ https://t.co/tXIFSsR12K https://t.co/law5XfRHTz\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "@canalplus, une rÃ©action officielle peut-Ãªtre sur les rÃ©vÃ©lations visant @PierreMenes ?... #PierreMenesOut #balancetonporc   #BalanceTonMenes #JeNeSuisPasUneSalope https://t.co/ParThES1UG\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "â€œJâ€™ai poussÃ© un cri de colÃ¨reâ€ : jugÃ©e en diffamation, la journaliste Sandra Muller Ã  lâ€™origine de #Balancetonporc plaide â€œla libÃ©ration de la parole des femmesâ€ https://t.co/nHix95DBmq\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "https://t.co/wFK3baer5V â€¦ dossier sur anissa jbr (achat de follower ,consomation de stup escorting) #jeremstarcensure   #BalanceTonPorc   #jeremstarcensure #babybel    #LaVilla3 #fiansosurskyrock  #JeremstarGate  #BALANCETONBABYBEL  #BalanceTonPorc  #Balancetatruie  #LPEPDLA\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@RC_ML pour continuer sur la vague de ce matin, la merveilleuse Nadia Daam @zappette parle, elle aussi, de son scepticisme de voir Mme Deneuve dans le mÃ©tro Ã  8h du mat. #moiaussi #MeToo #BalanceTonPorc https://t.co/Ns8vVKXAS0 :QT: \"Les amÃ©ricains ont Oprah Winfrey et des dÃ©bats sur Woody Allen, nous on a Elisabeth LÃ©vy, des dÃ©fenseurs du droit inaliÃ©nable Ã  coller des mains au cul, et des dÃ©bats sur les blagues de Tex !\"  RÃ©Ã©coutez le billet de Nadia Daam (@Zappette) #E1matin â¬‡ï¸ https://t.co/92svLYh4Vj\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@sychazot Et comme elle ne va pas Ãªtre nommÃ©e elle va encore instrumentaliser #metoo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@TPZ_Rap ... elle dÃ©nonce des choses ce qui a un lien avec le #balancetonporc ... la comparaison est pas nÃ©cessaire bref\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "On se demandera aussi si celles qui ont usÃ© de leurs atouts via promotion canapÃ© pour un poste parleront..#balancetonporc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Ã§a critique Pierre MÃ¨nes mais Ã§a klaxonne, siffle et traite les femmes quand Ã§a passe en bande dans une Peugeot 206 ðŸ’€ðŸ’€ðŸ’€ #PierreMenesOut #TPMP #balancetonporc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@SDJ_LCP a votÃ© suite Ã  la direction de remettre @frhaz Ã  l'antenne .. Ã©trangement Ã§a ne choque pas beaucoup de monde .. c'est fini #Balancetonporc ? OÃ¹ Ã§a vaut que pour certains #Haziza https://t.co/EOFckFfNbp\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Plaintes pour violences sexuelles (par le dessinateur Schvartz) -&gt; https://t.co/ymSfoZ7Wok #actualitÃ©endessins #JM #LOL #violencessexuelles #agressionssexuelles #violencesfaitesauxfemmes #harcÃ¨lementsexuel #paroledesfemmes #balanceTonPorc #schvartz #weinsteinscandal\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "#GisÃ¨leHalimi Ã©tait heureuse de voir qu'il y avait des ferments de rÃ©volte un peu partout   Exemple : #MeToo  Annick Cojean  ClÃ©a @Zenon8703 https://t.co/z3svClWRO6\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Vous pouvez Ã©galement le faire au commissariat ou gendarmerie le/la plus proche de chez vous #balancetonporc https://t.co/qWsG8KMISx\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "ErdoÄŸan le Dictateur islamiste !!! Insulte et amalgame volontaire avec des groupuscules terroristes. ErdoÄŸan les combats depuis des lustres alors que les prÃ©cÃ©dents les caressaient dans le sens du poil sans aucun gain !  #BalanceTonPastÄ±rma (contre attaque Ã  #BalanceTonPorc)\n",
      "0\n",
      "Label pour le tweet suivant :\n",
      "Un peu d'humour #balancetonporc ðŸ˜‚ https://t.co/eL4JX56m0f\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@FanJeanDujardin C bizarre de revoir #OSS117 au BrÃ©sil et ses rÃ©pliques sexistes qq mois aprÃ¨s #BalanceTonPorc .. Mais l'humour a tjs raison\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#BalanceTonPorc Squeezie avait donc raison\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@raphpradeau L'essentiel est qu'on laisse dans l'opacitÃ© le #MobbingEducationNationale et les violences sexuelles commises par des principaux et des proviseurs.  #metoo #moiaussi   #PasDeVague #OmertaEducatioNationale  https://t.co/ffGjK9kDQM\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#MeToo  Ã€ celui qui m'a embrassÃ© et frappÃ©.\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "Regardez le point de vue de @grosfilley et l'interview d'Anne Morelli, l'une des signataires de la tribune publiÃ©e hier dans Le Monde qui dÃ©nonce un certain fÃ©minisme qui exprimerait \"une certaine haine des hommes\" https://t.co/hxjPAAQ43T #Jour1 #MeToo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "RT @hrw au sujet du mouvement #MeToo en #Iran, oÃ¹ des voix s'Ã©lÃ¨vent pour demander une loi + forte concernant les violences contre les #femmes. https://t.co/W3yVudkzJq\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Ta copine de bac Ã  sable t'a volÃ© ta pelle quand t'avais 3 ans? Ta voisine de classe t'a pas laissÃ© copier sur sa dictÃ©e? Ta copine t'a quittÃ© alors que vous vous Ã©tiez promis l'amour Ã©ternel Ã  14 ans? DÃ©foule toi sur #Balancetapouffe OSEF. Les vrais pb sont lÃ  -&gt; #balancetonporc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Ce que #metoo a changÃ© pour les journalistes https://t.co/j1bwq1bcz9\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "#MetooAmnesie  #metooinceste  Tu aimes ton metier, tu es obligÃ© de t arrÃªter + de 3 mois car tu as peur, nausÃ©e, oppression, panique, flash, non-reconnaissance des gens en crises, concentration faible Et que ce n est pas de TA FAUTE! C est insupportable! SVP, Qui est concernÃ© ?\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Le chef du bureau de liaison dâ€™IsraÃ«l au Maroc dÃ©ment les accusations dâ€™abus sexuels #metoo https://t.co/TFeRW5lOPH\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "MÃªme HabillÃ© Avec Une Tunique XXL, Le Pervers Cherchera Toujours A Mater. #BalanceTonPorc\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "Ensemble, sortons du silence ! Appelez dÃ¨s maintenant le 0805 802 804 ou le 0800 100 811 depuis l'outre-mer du lundi au vendredi de 10h Ã  19h. TÃ©moignez pour vous. TÃ©moigner pour les enfants victimes. TÃ©moignez, tout simplement. @CIIVISE_contact  #Metooinceste #ViolenceSexuelles https://t.co/NoJky5WvM4\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Grande diffÃ©rence toutefois dans cette Ã©dition 2018, qui Ã©tait davantage placÃ©e sous le slogan #MeToo, plutÃ´t que #ResistTrump. https://t.co/9aSHEs85xZ\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Via @euronewsfr : Le producteur Harvey Weinstein exclu de l'AcadÃ©mie des Oscars https://t.co/WRV4LYbazT =Câ€™est tout ?#balancetonporc\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "@AlexDevecchio @PascalPraud le binÃ´me infÃ¢me. 2 ideologues dâ€™extreme droite raciste islamophobe. Pas content que â€˜les femmes franÃ§aises musulmanes nâ€™aient pas utilisÃ© le hashtag #balancetonporc â€˜ car elles auraient peur de se faire Ã©gorgÃ©s: ils sont journalistes, jamais inquiÃ¨tÃ©s https://t.co/C5DtnSXJBb :QT: ðŸ”´ðŸ‡«ðŸ‡·#FRANCE : Mensonges d'@AlexDevecchio (journaliste au #Figaro) qui dit, je cite \"elles se font Ã©gorgÃ©es en bas de leurs citÃ©s\".ðŸ§ #Mytho https://t.co/ORqkILKwn8\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#MeToo libÃ¨re la parole en #Egypte oÃ¹ le harcÃ¨lement sexuel reste encore tabou, explique le #NobelAlternatif @Mozn https://t.co/lgdzteIrhU\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "#balancetonporc  #balancetonporc qu'elle ambiance quand les hommes ne feront que rÃªver au bon vieux temps...\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#balancetonporc s'habiller en jean basket ds ton village car du + jeune au +++ Ã¢gÃ© tu es un morceau de viande!!! Ã€ vomir!\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "@TheDracerGx et mÃªme dâ€™un buddy movie avec Samuel L. Jackson, sur fond de revendications fÃ©ministes pour marquer un peu plus Hollywood dans lâ€™Ã¨re #MeToo.\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "On ne voit plus jm maire #tpmp petit problÃ¨me avec #balancetonporc?\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Chrronique de @sophiedurocher qui pisse sur le cadavre encore chaud dâ€™un pÃ¨re de 3 enfants.   Sophie veut surtout pas quâ€™on oublie #metoo dans le cas de Kobe mais elle est beaucoup plus silencieuse sur la cas de lâ€™ami Rozon. Les vacances Ã  Paris Ã§a forge des liens solides. https://t.co/gTXYYtqhR1\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#YoutubeusesDay ! Bravo mesdames !!! Svp (et je m'adresse aux femmes un minimum intelligentes). Vs vÃ©hiculez une influence sur les jeunes. Pourriez vs faire de la prÃ©vention contre la #pedocriminalite dans vos vidÃ©os/reseaux ? @innocencedanger #iwas   https://t.co/1PkpS3jKLJ\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "celles et ceux qui tweetent sur le #Iwas vous Ãªtes tlmt fort sachez le, vous mÃ©ritez dâ€™Ãªtre entendus\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Excellente analyse sur le trÃ¨s entendu \"#MeToo c'est bien, mais...\" https://t.co/tQvyCQX4wu\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Je rentre dans un magazin le gerant me suit \"pour me parler des produits \"et se met Ã  me toucher les seins #balancetonporc\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "C'est une agression sexuelle embraser une pets contre son consentement espece de porc @PierreMenes !!! Ã‡a toujours Ã©tÃ© une agression sexuelle complice infÃ¢me @canalplus ðŸ¤®ðŸ¤®ðŸ¤®ðŸ¤® #PierreMenesOut  #balancetonporc https://t.co/uALtjVQpoX :QT: Ah bon j'ai pas le droit de soulever des jupes et d'embrasser des femmes de forces ? Oh la la, le monde a changÃ© ! Gros dÃ©gueulasse va ðŸ¤® #TPMP #PierreMenesOut\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Marche aussi pour les mecs qui rÃ¢lent sur #balancetonporc https://t.co/8jj3aTp0mb\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Un magnifique projet trÃ¨s bientÃ´t thanks @ambreleguilly thanks @noemie.de.lattre hairstyle and makeup @nallahsangare #feminist #journeeinternationaledesdroitsdesfemmes #metoo #memepaspeur #resilience #sorority #model #ivorycoast #senegal #cotesdarmor #actress #writer #director https://t.co/LCm46glsJI\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#joyeusespaques #premieravril #jeremstargates #balancetonporc  #sondage vous faites quoi en premiers quand vous avez un kider surprise(follow,je fais des sondages)\n",
      "0\n",
      "Label pour le tweet suivant :\n",
      "@arthurberdah @ccornevin @AlbertZennou Il faut le stopper lui surtout le caillou dans la chaussure de Macron. Ã€ chaque fois quâ€™il ouvrira la bouche on pensera Ã  ses services rendus contre du sexe. #balancetonporc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Colo kayak, seule fille, une nuit tout le groupe avec les mono, mega groping sur moi dans mon duvet. J'ai jamais rien dit.  #balancetonporc\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "Bref, cette interview, nÃ©cessaire  Ã  mon sens, a confortÃ© mon avis sur cette affaire. Ã‡a n'en fait Ã©videmment pas un coupable, au mieux un mauvais communiquant en revanche. #PPDA #Quotidien @Qofficiel #MeToo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Ã€ un moment donnÃ©, les rÃ©alisateurs tv vont arrÃªter de faire des plans en contreplongÃ©e pour mater sous les jupes des filles ? ðŸ¤” #TPMP #BalanceTonPorc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Certaines ne parlent pas de peur de Â« dÃ©truire la vie Â» bien en place de bon pÃ¨re de famille insÃ©rÃ© de lâ€™agresseur, cela peut peut Ãªtre paraÃ®tre Ã©tonnant mais câ€™est un sentiment courant, cela renvoie dâ€™ailleurs aux propos trÃ¨s dÃ©placÃ©s qui ont eu cours lors de lâ€™Ã©pisode #metoo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@Space_Marcel @RHvitserk @crue_carmen @Le___Doc Ce qu'il faut que vous compreniez c'est le discours fÃ©ministe. Vous ne reconnaissez pas le systÃ¨me patriarcal dans lequel on vit ni la perspective de genre et forcÃ©ment, vous ne pouvez avoir aucune comprÃ©hension du mouvement #metoo autre que: elles veulent la guerre des sexes\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Catherine Deneuve et Brigitte Bardot moquÃ©es dans une Ã©mission amÃ©ricaine pour leurs positions sur #BalanceTonPorc https://t.co/gBQHbErWkR\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "CorÃ©e du Sud: une irrÃ©ductible de Samsung est la championne #MeToo https://t.co/JTEMJELsXg\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "#balancetonporc C'est pas gentil pour les cochons... ðŸ·ðŸ–\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#metoo oops #mytho...\"avant Ã§a passait crÃ¨me Cyril\" duxit Pierre MenÃ©s sur @Cyrilhanouna https://t.co/nx14Rz0ydT :QT: ðŸ‡«ðŸ‡· FLASH - Pierre #MÃ©nÃ¨s se dÃ©fend ce soir dans #TPMP : Â« Il faut prendre les gens comme ils sont, j'ai Ã©tÃ© embauchÃ© parce que je suis un personnage Â». Â« Si je ne peux plus chambrer une meuf parce que c'est une meuf, c'est insupportable Â», a-t-il ajoutÃ©. (interview) https://t.co/QLeIzByREW\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@PCF Pourquoi le parquet / procureur n'a pas poursuivi le violeur ?  Les juges doivent rÃ©pondre de leurs actes !   #responsabilitÃ© #pedocriminalitÃ© #metoo #balancetonporc #viols #Gailhaguet #justice\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@f_letellier @jeanvaljean2018 @GwennB_ Certes, aprÃ¨s, il y a des gens qui se consacrent Ã  plus dÃ©fendre, les femmes, ou les enfants ou, d autres, des associations qui se dÃ©vouent pour leur cause sans pour autant remettre en doute la parole des uns ou des autres comme Ã§a a Ã©tÃ© le cas pour #MeToo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Le #Iwas il me brise le coeur, vous Ãªtes tous/tes fort(e)s et extrÃªmement courageux(ses), je vous souhaite le meilleur ðŸ¥ºâ¤ï¸\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Le poulet a merite les coups de poings qu'il a pris car desole mais on est pas la pour prendre des coups donc ce sera coup pour coup La police a baisser dans l'estime des francais. #libertepourdettinger #balancetonporc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Merci Ã  @AbouDjaffar de RT #balancetonporc, Ã§a m'Ã©vite de cliquer sur le hashtag et d'en dÃ©couvrir encore plusâ€¦ #nausÃ©e\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@ta_malou @avec_marine Elle aime les enfants la preuve elle s en es occupÃ© aux lycÃ© de cette nouille de #macron brizitte devrais figurÃ© dans #BalanceTonPorc\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "A song against Violence / Un titre qui vous prend aux tripes \"Violent &amp; Happy\" de La Nouvelle Eve sur #SoundCloud ? #np https://t.co/FZiTWLPrlZ #violence #violenceconjugale #violenceathome #stayhome #music #resteralamaison #covid #corona #covid19 #bjork #bjÃ¶rk #metoo\n",
      "0\n",
      "Label pour le tweet suivant :\n",
      "Les tÃ©moignages du #MetooInceste sont juste glaÃ§ants. Les prÃ©dateurs sexuels ne sont jamais trÃ¨s loin .\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Donc le collectif #MeToo dÃ©montre sa bÃªtise abyssale et son manque de discernement. ðŸ˜‰ #QuandOnEstConOnEstCon https://t.co/DUGBqqLgqa\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@Lotisauteur @Mediapart Oui. J'ai Ã©tÃ© contactÃ© pour @BalanceTonPost et @TPMP notament et Ã  l'Ã©tranger pour parler de #MeToo. Quand je parlerai, je parlerai aussi de Ã§a\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Je viens de l'Ã©couter. Encore une fois, excellente intervention de @Ovidieofficiel qui a raison sur la vraie pollution du dÃ©bat (plus que nÃ©cessaire) ouvert par les #MeToo #Balancetonporc #TIMESUP .  Cette tribune du Monde ne fait que brouiller/salir toute vraie discussion. https://t.co/MVfkNqm3eB\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Ce matin, on rigole bien sur FB. Et tout Ã§a avant mon 2e cafÃ© #metoo #enitalien https://t.co/w6oA8YIHDo\n",
      "0\n",
      "Label pour le tweet suivant :\n",
      "Vatican. La derniÃ¨re bataille de BenoÃ®t XVI - Courrier International Blogs https://t.co/ILVd5Ry3XU #metoo https://t.co/5SoPW4oqaG\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "#metoo #metooinceste  \"prophet\":  4min24sec, pendant quelques secondes, il est comme De Niro parfois:   COMPLIQUE: je vous ecoute: je vous donne le pouvoir d'etre celui qui DOIT vous ecouter  A 16 ans, mes amis disait que j'etais comme lui dans \"Voyage au bout de l'enfer\",PARFOIS https://t.co/p42cEijVul\n",
      "0\n",
      "Label pour le tweet suivant :\n",
      "Si il est prouvÃ© que Sandra Muller  @LettreAudio  A DE GROS SEINS, il demeure incomprÃ©hensible qu'Eric Brion ai eu envie de se taper cette grosse truie #balancetonporc #journeemondialedesanimaux\n",
      "Ã©\n",
      "Label pour le tweet suivant :\n",
      "â€œEn allant vers l'hÃ©micycle,@jeanlassalle m'a mis une main aux fesses.â€œJulia Castanier se confie. #balancetonporc https://t.co/2ybbTRj5sq\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "@WiretteNicorett @fabfluckiger Quand tu dis que Ã§a se travaille, Ã  quel degrÃ© il est nÃ©cessaire de commencer un traitement ? Et avec qui ?  #MeToo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Quand on te demande ton 06 dans la rue, que tu dÃ©clines ou ignores pour finalement te faire insulter de pute... ðŸ˜© #balancetonporc\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "Ã€ 21h, le #doc Â« Beauvoir, lâ€™aventure dâ€™Ãªtre soi Â»  ðŸ‘©â€ðŸ« Ã€ lâ€™heure des dÃ©bats sur la domination masculine, la charge mentale, #MeToo, quelle est la modernitÃ© de Simone de Beauvoir ?  ðŸ‘‰ Avec LeÃ¯la Slimani, Elisabeth Badinter et @titiou Lecoq  RÃ©al Fabrice Gardel et @MathieuWeschler https://t.co/laqKrJzFBi\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Nouvelle plainte pour #viol contre #Tariqramadan #Ramadan #balancetonporc https://t.co/GMJT3S1oBM\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Donc chacun ne voit pas la mÃªme chose concernant 1er moitiÃ© du James Bond et deuxiÃ¨me moitiÃ© du film loque humaine merci #metoo https://t.co/3PLromOFdX\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "@Bloom12005028 Formidable merci. Oui il faut impÃ©rativement tout faire pour rÃ©former la lÃ©gislation sur le #viol au #Japon qui dÃ©jÃ  n'avait pas Ã©tÃ© modifiÃ©e, il y a peu, depuis 110 ans... #Scandale #MeToo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Le cas Aziz Ansari questionne intelligemment le mouvement #MeToo https://t.co/XWWojlDtJ2 https://t.co/hpL0NMZqwd\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "@CNEWS @angele_vl Une vraie rebelle cette meuf ! ðŸ˜† Chanson sur #balancetonporc pr surfer sur la vague, chanson sur le rÃ©chauffement en pleine canicule wow. J'annonce un prochain titre sur la pÃªche au homard et sur les vignettes Crit'Air mdrrr #risquezÃ©ro\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "\"Preuve que les questions liÃ©es aux femmes ont dominÃ© lâ€™annÃ©e, les mots-clefs les plus utilisÃ©s sur Twitter en 2018 place Â« school #MeToo Â», Â« feminist Â» et Â« molka Â»\".  Cet article date d'il y a un an, qu'en est-il pour 2019? ðŸ¤” #digipolUCLouvain https://t.co/FEyFo3NoDW\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#balancetonporc ce soir dans derniÃ¨re minute\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#Gilead ne lÃ¢che pas lâ€™affaire contre @raoult_didier  Ã‡a fait pls mois que c un feu roulant continu. Mais un truc comme Ã§a garantit des dommages collatÃ©raux ou alors il faudra un tri sÃ©lectif des dÃ©nonciations anonymes (DAs). AprÃ¨s les #metoo, Les DAs sont maintenant la norme. ðŸ¥¶ https://t.co/U9MZGxZ56K :QT: Dites-moi que c'est un fake! https://t.co/EM3WkFHBqo Y aurait-il des fonctionnaires visant des promotions en essayant de se faire bien voir en incitant Ã  la dÃ©lation.Opposer mÃ©decins et pharmaciens, pas forcÃ©ment une stratÃ©gie gagnante, et en tout cas pas au service des patients. https://t.co/ikc6v1MHSX\n",
      "0\n",
      "Label pour le tweet suivant :\n",
      "#Iwas 15, 16, 17, deux fois le mÃªme gros connard\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "2.2- #metoo #metooinceste   NOTES PÃ‰DOPHILES  Il a fallu que je me dÃ©barrasse de le personne qui a peut-Ãªtre le plus de nonSoi que je connaisse et qui me pose beaucoup trop de problÃ¨me: j'ai dÃ©montÃ© son ramassis de prÃ©tention et d'apparences ridicules.\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Agression et harcÃ¨lement sexuels: le photographe star Patrick Demarchelier accusÃ©, Anna Wintour mise en cause https://t.co/4Z3wnQTXWy #metoo #Demarchelier #Wintour https://t.co/M2Kbd93WtK\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "Un an aprÃ¨s #MeToo, retrouvons-nous pour dire ðŸ›‘ STOP aux violences  sexistes et sexuelles. RDV samedi 29 septembre, Ã  14h30, place de la  RÃ©publique, pour un die-in gÃ©ant.  ðŸ“… Ã‰vÃ©nement Facebook : https://t.co/eVUsV1tczD ðŸ‘‰ Inscriptions : https://t.co/y0dJaQCalu https://t.co/gNnUP7YzXt\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#balancetonporc, Ã§a balance pas mal Ã  Paris, Ã§a balance pas mÃ¢leâ€¦ https://t.co/EFyUSn9Pf8 https://t.co/LlpDsgYjbH\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "J'avais 2 ans. C'Ã©tait le fils d'un ami de la famille : il en avait entre 11 et 13. Je l'ai recroisÃ© il y a 3 ans : il a maintenant 40 ans et m'a regardÃ© en mode \"tu te souviens\" avec son regard de pervers.  J'ai portÃ© la culpabilitÃ© de son acte pendant 30 ans. #metooinceste\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "Avec mon bÃ©bÃ© de 8 mois dans son landeau.Un type approche:c'est une fille ou un garÃ§on? Moi:une fille.Lui:une future suceuse #balancetonporc\n",
      "3\n",
      "Label pour le tweet suivant :\n",
      "Le prÃªtre de Trois-RiviÃ¨res a tentÃ© de mettre fin Ã  ses jours - RCI https://t.co/8Zsk7oQnz9 #metoo https://t.co/r0dkl00Pla\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "Des suffragettes Ã  #MeToo : le violet, couleur iconique des mouvements fÃ©ministes https://t.co/Jeu0Cb216y @letemps #soc\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "#Metoo Les fÃ©ministes radicales de gauche de plus en plus critiquÃ©es par des...fÃ©ministes....https://t.co/hQLAbzqHDg via @lp_lapresse\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "@YannisPSD @_Ecce__Homo_ @MarleneSchiappa \"#SÃ©nÃ©galðŸ‡¸ðŸ‡³ : #MacronðŸ‡«ðŸ‡· et #Rihanna saluent un nouvel Ã©lan pour lâ€™Ã©ducation\"  #Schiappa @Elysee #BrigitteMacron #France #Paris #LREM #EnMarche #LaREM #BalanceTonPorc https://t.co/BPJXRAnkNa\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Hanouna avec capu anav et lors de lâ€™entretien d embauche d enora #balancetonporc@hanouna@c8\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "ðŸ”´Notre tribune dans @libe  âž¡ï¸\"Violences Sexuelles sur mineur.e.s : lâ€™heure de la tolÃ©rance zÃ©ro a sonnÃ©\"ðŸ””   #justicepourjulie #metooinceste  Avec @memoiretrauma @MiKohiyama @AndreaBescond @Abitbol_sarah @corinne_leriche @aludv P. Ã‰. Germain Thill  ðŸ“Œ https://t.co/tSxr6LHm28 https://t.co/qpTnlxJ2BS\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "https://t.co/Ezz6jd3MjO Oui je l'adore ðŸ’™ðŸŽµðŸŽ¶ Je ne suis pas #MeToo https://t.co/QTzSFXWAFH\n",
      "1\n",
      "Label pour le tweet suivant :\n",
      "@MiKohiyama Pas que dans le sport, pas que dans le cinÃ©ma, les porcs ne sont pas tous sous les projecteurs mais ils sont partout dans nos vies de tous les jours, tout prÃ¨s et les victimes aussi...ðŸ˜¢#metoo\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Pourquoi certaines enquÃªtes sont-elles d'utilitÃ© publique ? Pour que des anonymes ne s'autorisent plus de traiter des victimes potentielles de \"femelles\" hystÃ©riques. Les enquÃªtes permettent d'ouvrir des enquÃªtes, qui dÃ©montrerons ou non la culpabilitÃ© ou l'innocence. #MeToo https://t.co/06RIyrKGBG\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "Bordel j'suis dans l'EHPAD de mon grand-pÃ¨re.. putain si les soignantes participaient au hashtag #balancetonporc mdddddr Ã§a aurait saturer Twitter\n",
      "2\n",
      "Label pour le tweet suivant :\n",
      "#Ã©vÃ©nement RÃ©putation Time 2017 : du tweet #balancetonporc Ã  la couverture du Elle, anatomie dâ€™une levÃ©e de boucliers gÃ©nÃ©ralisÃ©e. C'est demain avec @annatwit #RepTime2017  En savoir plus &gt; https://t.co/Z0nGlUq457  Programme et inscription &gt; https://t.co/gP3b4GM9CP https://t.co/V6BsQrFcnR\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "labeliser_tweet(df, nb_tweets=100, random_state = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "jAyzaJ5875dw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_label = pd.read_csv('label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_label = df_label.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_label = df_label.drop(1979)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il n'y a pas de chaÃ®nes de caractÃ¨res avec des lettres dans la colonne 'label'.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Filtrer les chaÃ®nes de caractÃ¨res contenant des lettres dans la colonne \"label\"\n",
    "has_strings = df_label['label'].apply(lambda x: bool(re.search('[a-zA-Z]', str(x))))\n",
    "\n",
    "if has_strings.any():\n",
    "    # Supprimer les lignes contenant des chaÃ®nes de caractÃ¨res avec des lettres dans la colonne \"label\"\n",
    "    df_label = df_label[~has_strings]\n",
    "    print(\"Les lignes contenant des chaÃ®nes de caractÃ¨res avec des lettres ont Ã©tÃ© supprimÃ©es.\")\n",
    "else:\n",
    "    print(\"Il n'y a pas de chaÃ®nes de caractÃ¨res avec des lettres dans la colonne 'label'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_label.label = [int(label) for label in df_label.label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "mRUK1kmZ7_z8",
    "outputId": "78d2f214-ea47-47f1-d388-9679b8d6d384",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    1331\n",
       "1     412\n",
       "3     182\n",
       "0      81\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "hnt-ro2_WpXz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining constants\n",
    "epochs = 50\n",
    "MAX_LEN = 300\n",
    "batch_size = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "-JvbX630WUHn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertForSequenceClassification(\n",
       "  (roberta): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): CamembertClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=4)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=10e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "cQS2l-oQZwmL",
    "outputId": "ab347867-6f13-41ee-9457-1ea3ef877c2f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "text = df_label['text'].to_list()\n",
    "labels = df_label['label'].to_list()\n",
    "\n",
    "#user tokenizer to convert sentences into tokenizer\n",
    "input_ids  = [tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN) for sent in text]\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]  \n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "Ge4RCSKcZztV",
    "outputId": "47bd58fd-bb54-45c5-b011-e4af769fcee9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "                                                            random_state=42, test_size=0.4, stratify = labels)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9748553665060746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|â–         | 1/50 [05:49<4:45:30, 349.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6665178571428572\n",
      "Train loss: 0.8132788131111547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   4%|â–         | 2/50 [12:09<4:54:09, 367.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7814903846153847\n",
      "Train loss: 0.636475898717579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   6%|â–Œ         | 3/50 [19:07<5:05:42, 390.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.832760989010989\n",
      "Train loss: 0.504281925527673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   8%|â–Š         | 4/50 [25:38<4:59:30, 390.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8774038461538461\n",
      "Train loss: 0.39367974118182536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|â–ˆ         | 5/50 [32:15<4:54:46, 393.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8393543956043956\n",
      "Train loss: 0.32853785941475316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  12%|â–ˆâ–        | 6/50 [38:56<4:50:15, 395.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.860989010989011\n",
      "Train loss: 0.27336769041262177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  14%|â–ˆâ–        | 7/50 [45:21<4:41:03, 392.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8718063186813187\n",
      "Train loss: 0.23118420023667186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  16%|â–ˆâ–Œ        | 8/50 [51:37<4:30:57, 387.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8882211538461539\n",
      "Train loss: 0.207986234050048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  18%|â–ˆâ–Š        | 9/50 [58:02<4:23:58, 386.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8762019230769231\n",
      "Train loss: 0.17864405167730232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|â–ˆâ–ˆ        | 10/50 [1:04:24<4:16:40, 385.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8573832417582418\n",
      "Train loss: 0.14990130459007464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  22%|â–ˆâ–ˆâ–       | 11/50 [1:10:39<4:08:20, 382.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8657967032967033\n",
      "Train loss: 0.14182989220870168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  24%|â–ˆâ–ˆâ–       | 12/50 [1:17:14<4:04:27, 385.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8918269230769231\n",
      "Train loss: 0.1237568037682458\n",
      "Train loss: 0.10782157904223393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  28%|â–ˆâ–ˆâ–Š       | 14/50 [1:29:59<3:50:10, 383.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8657967032967033\n",
      "Train loss: 0.11725765270622153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|â–ˆâ–ˆâ–ˆ       | 15/50 [1:35:32<3:34:52, 368.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8778159340659341\n",
      "Train loss: 0.11720255762338638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [1:42:14<3:34:30, 378.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.817135989010989\n",
      "Train loss: 0.10115535459236095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [1:48:49<3:51:15, 408.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8740041208791209\n",
      "Early stopping! No improvement in validation loss for 10 consecutive epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss_set = []\n",
    "best_validation_loss = float('inf')\n",
    "epochs_no_improve = 0  # nombre d'epochs consÃ©cutives sans amÃ©lioration de la validation\n",
    "best_model = None  # stockage du meilleur modÃ¨le\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels.long())\n",
    "        loss = outputs[0]\n",
    "        train_loss_set.append(loss.item())    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "    # Evaluate on validation data\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels.long())\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "    \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "            nb_eval_examples += b_input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "\n",
    "    # Check for early stopping\n",
    "    validation_loss = eval_loss/nb_eval_steps\n",
    "    if validation_loss < best_validation_loss - 0.01:\n",
    "        best_validation_loss = validation_loss\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        # Enregistrer le meilleur modÃ¨le\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve == 10:\n",
    "        print(\"Early stopping! No improvement in validation loss for 10 consecutive epochs.\")\n",
    "        break\n",
    "\n",
    "# Enregistrer le meilleur modÃ¨le sur le disque\n",
    "if best_model is not None:\n",
    "    torch.save(best_model, \"best_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(\"best_model.pth\", \"classif_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertForSequenceClassification(\n",
       "  (roberta): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): CamembertClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=4)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    # mettre le modÃ¨le en mode d'Ã©valuation\n",
    "    model.eval()\n",
    "    \n",
    "    # stocker les prÃ©dictions de tous les batchs dans une liste\n",
    "    predictions = []\n",
    "    \n",
    "    # boucle sur les batches dans le dataloader de test\n",
    "    for batch in dataloader:\n",
    "        \n",
    "        # dÃ©placer les donnÃ©es sur le mÃªme dispositif que le modÃ¨le\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # dÃ©sactiver le calcul des gradients pour Ã©conomiser de la mÃ©moire et accÃ©lÃ©rer les calculs\n",
    "        with torch.no_grad():\n",
    "            # faire passer les donnÃ©es Ã  travers le modÃ¨le pour obtenir les logits\n",
    "            outputs =  model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = outputs[0]\n",
    "    \n",
    "        # appliquer la fonction softmax pour obtenir les probabilitÃ©s pour chaque classe\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # convertir les probabilitÃ©s en prÃ©dictions en prenant l'indice de la classe avec la probabilitÃ© la plus Ã©levÃ©e\n",
    "        batch_preds = torch.argmax(probs, dim=1)\n",
    "        \n",
    "        # ajouter les prÃ©dictions pour ce batch Ã  la liste de prÃ©dictions globale\n",
    "        predictions.extend(batch_preds.cpu().numpy().tolist())\n",
    "    \n",
    "    # retourner la liste de prÃ©dictions globale\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_val = predict(model, validation_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(validation_labels.numpy(), return_counts=True)\n",
    "label_counts = dict(zip(unique, counts))\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(validation_labels, predictions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = ['Autre','Presse','Opinion','TÃ©moignage'], \n",
    "                     columns = ['Autre','Presse','Opinion','TÃ©moignage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_df, annot=True, cbar = False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('')\n",
    "plt.xlabel('PrÃ©dictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Charger les donnÃ©es et les mettre au format appropriÃ©\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[43mtexts\u001b[49m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "# Charger les donnÃ©es et les mettre au format appropriÃ©\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "# InfÃ©rer les prÃ©dictions et les probabilitÃ©s de chaque classe\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs[0]\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    preds = torch.argmax(logits, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter une colonne de prÃ©dictions\n",
    "df[\"predictions\"] = preds.cpu().numpy()\n",
    "\n",
    "# Ajouter des colonnes de probabilitÃ©s pour chaque classe\n",
    "for i, label in enumerate(label_list):\n",
    "    df[label] = probs[:, i].cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "pbFcqTLYWnIZ",
    "outputId": "ca5a0099-dba7-4b4e-ad98-14b46dbb3819",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2618875404198964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|â–ˆ         | 1/10 [11:58<1:47:46, 718.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.683666087962963\n",
      "Train loss: 1.0668712556362152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|â–ˆâ–ˆ        | 2/10 [26:31<1:47:53, 809.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.683666087962963\n",
      "Train loss: 0.957703024148941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [41:12<1:38:15, 842.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.683666087962963\n",
      "Train loss: 0.8441829631725947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [56:13<1:26:33, 865.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7936197916666666\n",
      "Train loss: 0.7229040463765463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [1:11:12<1:13:06, 877.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7316261574074074\n",
      "Train loss: 0.6163275490204493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [1:25:21<57:51, 867.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7779947916666666\n",
      "Train loss: 0.5189388146003088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [1:38:30<42:06, 842.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8236400462962963\n",
      "Train loss: 0.4161665042241414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [1:46:59<24:32, 736.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8298611111111112\n",
      "Train loss: 0.3496982256571452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [1:54:01<10:37, 637.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8383969907407407\n",
      "Train loss: 0.2971478613714377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [2:00:39<00:00, 723.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8423032407407407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### sans early stopping\n",
    "train_loss_set=[]\n",
    "for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels.long())\n",
    "        loss = outputs[0]\n",
    "        train_loss_set.append(loss.item())    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    model.eval()\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs =  model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels.long())\n",
    "            loss, logits = outputs[:2]\n",
    "    \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
