{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kV9SWKZSWAh0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 17:11:15.075423: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-06 17:11:15.135975: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-06 17:11:15.876802: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForSequenceClassification, CamembertForMaskedLM, AutoTokenizer, AutoConfig\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "# Importing specific libraries for data prerpcessing, model archtecture choice, training and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "95lkuND-8fA-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def labeliser_tweet(df, nb_tweets=None, random_sample=False):\n",
    "\n",
    "    # Charger le fichier \"label.csv\" pour v√©rifier les tweets d√©j√† labelis√©s\n",
    "    tweets_labelises = set()\n",
    "    try:\n",
    "        with open('label.csv', 'r') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                tweets_labelises.add(row['tweet_id'])\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    # S√©lectionner les tweets √† labeliser\n",
    "    if random_sample:\n",
    "        if nb_tweets:\n",
    "            tweets_a_labeliser = df.drop(tweets_labelises).sample(n=nb_tweets)\n",
    "        else:\n",
    "            tweets_a_labeliser = df.drop(tweets_labelises)\n",
    "    else:\n",
    "        if nb_tweets:\n",
    "            tweets_a_labeliser = df.drop(tweets_labelises).head(nb_tweets)\n",
    "        else:\n",
    "            tweets_a_labeliser = df.drop(tweets_labelises)\n",
    "\n",
    "    # Ouvrir le fichier \"label.csv\" en mode ajout, pour ne pas √©craser les donn√©es pr√©c√©dentes\n",
    "    with open('label.csv', 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        # Si le fichier est vide, ajouter une ligne d'en-t√™te\n",
    "        if csvfile.tell() == 0:\n",
    "            writer.writerow(['tweet_id', 'text', 'label'])\n",
    "\n",
    "        # Boucle pour labeliser chaque tweet s√©lectionn√©\n",
    "        for tweet_id, row in tweets_a_labeliser.iterrows():\n",
    "            text = row['text']\n",
    "            labelisation = input(f'Label pour le tweet \"{text}\": ')\n",
    "\n",
    "            # √âcrire la labelisation dans le fichier \"label.csv\"\n",
    "            writer.writerow([tweet_id, text, labelisation])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vtLTsuk8BkZ3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def labeliser_tweet(df, nb_tweets, label_file):\n",
    "    # Charger les tweets d√©j√† labelis√©s\n",
    "    try:\n",
    "        with open(label_file, mode='r') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            tweets_labelises = set(row['tweet_id'] for row in reader)\n",
    "    except FileNotFoundError:\n",
    "        tweets_labelises = set()\n",
    "\n",
    "    # S√©lectionner les tweets √† labeliser\n",
    "    df_a_labeliser = df[~df.index.isin(tweets_labelises)].sample(n=nb_tweets, random_state=42)\n",
    "\n",
    "    # It√©rer sur les tweets s√©lectionn√©s et demander le label √† l'utilisateur\n",
    "    for tweet_id, row in df_a_labeliser.iterrows():\n",
    "        print(row['text'])\n",
    "        labelisation = input(f'Label pour {text}: ')\n",
    "        df.loc[tweet_id, 'label'] = labelisation\n",
    "\n",
    "    # Sauvegarder les labels dans le fichier CSV\n",
    "    df.to_csv(label_file, index_label='tweet_id')\n",
    "\n",
    "    print(f\"{nb_tweets} tweets ont √©t√© labelis√©s et ajout√©s au fichier {label_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pADt5CykCKaz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def labeliser_tweet(df, nb_tweets=5):\n",
    "    \n",
    "    # V√©rifier si le fichier \"label.csv\" existe et charger les tweet_id d√©j√† labelis√©s\n",
    "    tweets_labelises = set()\n",
    "    try:\n",
    "        with open('label.csv', mode='r') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                tweets_labelises.add(row['tweet_id'])\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "    # S√©lectionner un √©chantillon al√©atoire de tweets non encore labelis√©s\n",
    "    df_a_labeliser = df[~df.index.isin(tweets_labelises)].sample(n=nb_tweets, random_state=42)\n",
    "    \n",
    "    # Labeliser les tweets s√©lectionn√©s et enregistrer les labels dans un fichier CSV\n",
    "    with open('label.csv', mode='a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if f.tell() == 0:\n",
    "            writer.writerow(['tweet_id', 'text', 'label'])\n",
    "        for tweet_id, text in df_a_labeliser['text'].items():\n",
    "            label = input(f'Label pour le tweet suivant :\\n{text}\\n')\n",
    "            # √âcrire les donn√©es labelis√©es dans le fichier CSV\n",
    "            writer.writerow([tweet_id, text, label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "fGUP7YlcanuW",
    "outputId": "9d6da307-a96e-45dc-ca7f-acd015856f90",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('base_annotation_fev23.csv', index_col='tweet_id').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKjOXzht-kvr",
    "outputId": "18bd519c-92df-4ce6-c15c-ec4555f02a8c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Label pour le tweet suivant :\n",
      "Vous ne connaissez pas de coll√®gues qui ont √©t√© viol√©es en toute impunit√© par un sup√©rieur hi√©rarchique, donc ?  Mince. Mais vous pouvez peut-√™tre vous informer, non ? Ce n'est pas la premi√®re fois que je vous le demande sur Twitter. üôâüôàüôä  #metoo  #moiaussiEducationNationale https://t.co/R1IqhWgQKl\n",
      " 0\n",
      "Label pour le tweet suivant :\n",
      "-Jsais profiter de ma notori√©t√© pour agresser sexuellement qui jveux puis pr√©tendre √† l'absence d'humour chez mes victimes pour l√©gitimer mes actes!  -Bienvenue chez Canal+ !!!!!  #balancetonporc   #PierreMenesOut https://t.co/0YyApNM3jh\n",
      " 0\n",
      "Label pour le tweet suivant :\n",
      "#metoo #metooinceste  Excusez-moi, j'avais pr√©vu un texte si je craquerais.\n",
      " 0\n",
      "Label pour le tweet suivant :\n",
      "#MeToo Je zappe : les femmes d√©fendues √† la t√©l√©vision par un imb√©cile presqu'assexu√©, c'est trop fort... Hey, les godiches, vous vous prenez pour qui ? QUI ?\n",
      " 0\n",
      "Label pour le tweet suivant :\n",
      "#metooinceste #NousToutes @memoiretrauma  Je n'avais pas d'√¢ge. Mon p√®re a viol√© ma m√®re et moi dans son ventre pr√™te √† voir la lumi√®re. Je suis n√©e la t√™te meurtrie, souill√©e, avec une m√®re sid√©r√©e, dissoci√©e dans mes bras. Quelques mois apr√®s il m'a viol√©e, j'√©tais un b√©b√©.\n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "labeliser_tweet(df, nb_tweets=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jAyzaJ5875dw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_label = pd.read_csv('label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "mRUK1kmZ7_z8",
    "outputId": "78d2f214-ea47-47f1-d388-9679b8d6d384",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1282587829357490179</td>\n",
       "      <td>Vous ne connaissez pas de coll√®gues qui ont √©t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1374666678533951488</td>\n",
       "      <td>-Jsais profiter de ma notori√©t√© pour agresser ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463447795877265410</td>\n",
       "      <td>#metoo #metooinceste  Excusez-moi, j'avais pr√©...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1305966401887371264</td>\n",
       "      <td>#MeToo Je zappe : les femmes d√©fendues √† la t√©...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1350452454782070786</td>\n",
       "      <td>#metooinceste #NousToutes @memoiretrauma  Je n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1282587829357490179</td>\n",
       "      <td>Vous ne connaissez pas de coll√®gues qui ont √©t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1374666678533951488</td>\n",
       "      <td>-Jsais profiter de ma notori√©t√© pour agresser ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1463447795877265410</td>\n",
       "      <td>#metoo #metooinceste  Excusez-moi, j'avais pr√©...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1305966401887371264</td>\n",
       "      <td>#MeToo Je zappe : les femmes d√©fendues √† la t√©...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1350452454782070786</td>\n",
       "      <td>#metooinceste #NousToutes @memoiretrauma  Je n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1282587829357490179  Vous ne connaissez pas de coll√®gues qui ont √©t...   \n",
       "1  1374666678533951488  -Jsais profiter de ma notori√©t√© pour agresser ...   \n",
       "2  1463447795877265410  #metoo #metooinceste  Excusez-moi, j'avais pr√©...   \n",
       "3  1305966401887371264  #MeToo Je zappe : les femmes d√©fendues √† la t√©...   \n",
       "4  1350452454782070786  #metooinceste #NousToutes @memoiretrauma  Je n...   \n",
       "5  1282587829357490179  Vous ne connaissez pas de coll√®gues qui ont √©t...   \n",
       "6  1374666678533951488  -Jsais profiter de ma notori√©t√© pour agresser ...   \n",
       "7  1463447795877265410  #metoo #metooinceste  Excusez-moi, j'avais pr√©...   \n",
       "8  1305966401887371264  #MeToo Je zappe : les femmes d√©fendues √† la t√©...   \n",
       "9  1350452454782070786  #metooinceste #NousToutes @memoiretrauma  Je n...   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      0  \n",
       "6      0  \n",
       "7      0  \n",
       "8      0  \n",
       "9      0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "hnt-ro2_WpXz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining constants\n",
    "epochs = 5\n",
    "MAX_LEN = 128\n",
    "batch_size = 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-JvbX630WUHn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f41dc8c0d84f0d8be0d8ce48a91ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertForSequenceClassification(\n",
       "  (roberta): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): CamembertClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=4)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=10e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "cQS2l-oQZwmL",
    "outputId": "ab347867-6f13-41ee-9457-1ea3ef877c2f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = df_label['text'].to_list()\n",
    "labels = df_label['label'].to_list()\n",
    "\n",
    "#user tokenizer to convert sentences into tokenizer\n",
    "input_ids  = [tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN) for sent in text]\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]  \n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "Ge4RCSKcZztV",
    "outputId": "47bd58fd-bb54-45c5-b011-e4af769fcee9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "                                                            random_state=42, test_size=0.1)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "pbFcqTLYWnIZ",
    "outputId": "ca5a0099-dba7-4b4e-ad98-14b46dbb3819",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|‚ñà‚ñà        | 1/5 [00:01<00:06,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.3114666938781738\n",
      "Validation Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:03<00:04,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2596677541732788\n",
      "Validation Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:04<00:02,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2049448490142822\n",
      "Validation Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:05<00:01,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1606032848358154\n",
      "Validation Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:06<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1109304428100586\n",
      "Validation Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss_set=[]\n",
    "for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels.long())\n",
    "        loss = outputs[0]\n",
    "        train_loss_set.append(loss.item())    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    model.eval()\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs =  model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels.long())\n",
    "            loss, logits = outputs[:2]\n",
    "    \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgii8LE3Y6_N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
